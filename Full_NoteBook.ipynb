{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPAAE-bfLtKR",
        "outputId": "8ccd7e3e-1955-4918-b9cf-379fdaa4683e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gurobipy in /usr/local/lib/python3.11/dist-packages (12.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gurobipy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "\"WLSACCESSID\": '3bb2aa05-fbf2-40de-9010-2927967a3661',\n",
        "\"WLSSECRET\": 'fcc26b83-866c-418a-971d-650e68870b36',\n",
        "\"LICENSEID\": 2652745,\n",
        "\"Threads\": 95,\n",
        "\"NumericFocus\":0,\n",
        "}\n"
      ],
      "metadata": {
        "id": "Hm6T3mdJL0D8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aufRumJzMD1t"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gurobipy as gp\n",
        "from gurobipy import GRB\n",
        "from typing import List, Dict, Tuple, Optional, Any, Set # Added Set for Label\n",
        "\n",
        "# Placeholder for Label class and ESPPRC functions if not defined elsewhere\n",
        "# For this snippet to be runnable, you'd need:\n",
        "# - Label class definition\n",
        "# - initialize_depot_label(data, is_forward)\n",
        "# - solve_espprc_forward_labeling(data, dual_prices, alpha_penalty, depot_return_time_limit)\n",
        "# - solve_espprc_bidirectional(data, dual_prices, alpha_penalty) -> though current wrapper uses forward\n",
        "# - calculate_distance_matrix(coordinates_data)\n",
        "# - calculate_travel_time_matrix(distances, speed)\n",
        "# These would typically be in the same file or imported.\n",
        "\n",
        "def solve_restricted_master_problem(current_routes: List[Dict[str, Any]],\n",
        "                                    customers_list: List[int],\n",
        "                                    alpha_penalty: float,\n",
        "                                    model_name: str = \"RMP\") -> Tuple[Optional[gp.Model], Optional[Dict[int, float]]]:\n",
        "    \"\"\"\n",
        "    Solves the LP relaxation of the Restricted Master Problem.\n",
        "\n",
        "    Args:\n",
        "        current_routes: List of route dictionaries. Each route dict should contain:\n",
        "                        'id': A unique route identifier.\n",
        "                        'cost': The travel distance of the route.\n",
        "                        'customers_served': A list of customer IDs served by this route.\n",
        "        customers_list: List of all customer IDs.\n",
        "        alpha_penalty: Penalty for using a vehicle/route.\n",
        "        model_name: Name for the Gurobi model.\n",
        "\n",
        "    Returns:\n",
        "        A tuple (model, dual_prices). dual_prices is a dict {customer_id: dual_value}.\n",
        "        Returns (None, None) if solving fails.\n",
        "    \"\"\"\n",
        "    env = gp.Env(params=params)\n",
        "    rmp = gp.Model(model_name, env = env)\n",
        "    #rmp.setParam('OutputFlag', 0)  # Suppress Gurobi output\n",
        "\n",
        "    route_vars: Dict[str, gp.Var] = {}\n",
        "\n",
        "    # Define variables and objective\n",
        "    for idx, route_details in enumerate(current_routes):\n",
        "        route_id = route_details.get('id', f\"route_{idx}\")\n",
        "        # Cost C_r = (travel distance of route r) + alpha_penalty\n",
        "        cost_r = route_details['cost'] + alpha_penalty\n",
        "        route_vars[route_id] = rmp.addVar(obj=cost_r, vtype=GRB.CONTINUOUS, lb=0.0, name=f\"lambda_{route_id}\")\n",
        "\n",
        "    rmp.modelSense = GRB.MINIMIZE\n",
        "\n",
        "    # Define constraints (Set Covering: sum(a_ir * lambda_r) >= 1 for each customer i)\n",
        "    customer_constraints: Dict[int, gp.Constr] = {}\n",
        "    for i in customers_list:\n",
        "        # a_ir is 1 if customer i is in route_details['customers_served']\n",
        "        expr = gp.quicksum(route_vars[r_details.get('id', f\"route_{r_idx}\")]\n",
        "                           for r_idx, r_details in enumerate(current_routes)\n",
        "                           if i in r_details['customers_served'])\n",
        "\n",
        "        # Only add constraint if the customer is actually served by at least one route in the current pool\n",
        "        # This avoids issues with empty expressions if a customer is not yet covered.\n",
        "        if expr.size() > 0:\n",
        "            customer_constraints[i] = rmp.addConstr(expr >= 1, name=f\"cover_cust_{i}\")\n",
        "        else:\n",
        "            # If a customer is not covered by any route in the pool, its dual price is effectively\n",
        "            # what the ESPPRC needs to overcome. Gurobi won't assign a .Pi to a non-existent constraint.\n",
        "            # We'll handle this when constructing dual_prices.\n",
        "            pass\n",
        "\n",
        "\n",
        "    try:\n",
        "        rmp.optimize()\n",
        "    except gp.GurobiError as e:\n",
        "        print(f\"Gurobi error during RMP optimization: {e}\")\n",
        "        return None, None\n",
        "\n",
        "    if rmp.status == GRB.OPTIMAL:\n",
        "        dual_prices: Dict[int, float] = {}\n",
        "        for i in customers_list:\n",
        "            if i in customer_constraints: # Constraint exists for this customer\n",
        "                dual_prices[i] = customer_constraints[i].Pi\n",
        "            else: # Customer not covered by any route in current RMP pool\n",
        "                  # The \"true\" dual can be thought of as very high, or simply 0 if no constraint.\n",
        "                  # ESPPRC should still try to find paths for such customers.\n",
        "                  # Setting to 0 is a common practice if constraint doesn't exist.\n",
        "                dual_prices[i] = 0.0\n",
        "        return rmp, dual_prices\n",
        "    else:\n",
        "        print(f\"RMP optimization failed with status: {rmp.status}\")\n",
        "        # Attempt to retrieve duals if model is infeasible but duals might exist (e.g. IIS)\n",
        "        # This is advanced and depends on Gurobi's state. For simplicity, return None.\n",
        "        return None, None\n",
        "\n",
        "\n",
        "def solve_master_ip(final_routes: List[Dict[str, Any]],\n",
        "                    customers_list: List[int],\n",
        "                    alpha_penalty: float,\n",
        "                    model_name: str = \"MasterIP\") -> Tuple[Optional[gp.Model], Optional[List[Dict[str, Any]]]]:\n",
        "    \"\"\"\n",
        "    Solves the Master Problem as an Integer Program with the final set of routes.\n",
        "    \"\"\"\n",
        "    # Create a new Gurobi environment and model\n",
        "    # Using default environment is usually fine unless specific parameters are needed globally\n",
        "    env = gp.Env(params=params)\n",
        "    master_ip = gp.Model(model_name)\n",
        "    #master_ip.setParam('OutputFlag', 0) # Suppress Gurobi output by default\n",
        "\n",
        "    route_vars: Dict[str, gp.Var] = {}\n",
        "\n",
        "    if not final_routes:\n",
        "        print(\"Master IP warning: No routes provided in final_routes pool.\")\n",
        "        return master_ip, [] # Return empty solution\n",
        "\n",
        "    for idx, route_details in enumerate(final_routes):\n",
        "        route_id = route_details.get('id', f\"route_{idx}\")\n",
        "        cost_r = route_details['cost'] + alpha_penalty\n",
        "        route_vars[route_id] = master_ip.addVar(obj=cost_r, vtype=GRB.BINARY, name=f\"lambda_{route_id}\")\n",
        "\n",
        "    master_ip.modelSense = GRB.MINIMIZE\n",
        "\n",
        "    # Ensure all customers can be covered by at least one route in the pool\n",
        "    for i in customers_list:\n",
        "        expr = gp.quicksum(route_vars[r_details.get('id', f\"route_{r_idx}\")]\n",
        "                           for r_idx, r_details in enumerate(final_routes)\n",
        "                           if i in r_details['customers_served'])\n",
        "        if expr.size() > 0:\n",
        "            master_ip.addConstr(expr >= 1, name=f\"cover_cust_{i}\")\n",
        "        else:\n",
        "            # This customer cannot be served by any route in the final pool.\n",
        "            # The IP will be infeasible. This indicates an issue in the column generation.\n",
        "            print(f\"ERROR in Master IP: Customer {i} cannot be served by any route in the final pool. IP will likely be infeasible.\")\n",
        "            # To allow the model to solve for other customers, you might skip this constraint,\n",
        "            # or the problem is ill-defined if a customer has no routes.\n",
        "            # For now, we add it, which will lead to infeasibility if expr is empty.\n",
        "            master_ip.addConstr(expr >= 1, name=f\"cover_cust_{i}\") # This will make it infeasible if expr is empty.\n",
        "\n",
        "    try:\n",
        "        master_ip.optimize()\n",
        "    except gp.GurobiError as e:\n",
        "        print(f\"Gurobi error during Master IP optimization: {e}\")\n",
        "        return master_ip, None # Return model for inspection\n",
        "\n",
        "    selected_routes_details: List[Dict[str, Any]] = []\n",
        "    if master_ip.status == GRB.OPTIMAL or \\\n",
        "       (master_ip.status in [GRB.TIME_LIMIT, GRB.INTERRUPTED, GRB.SUBOPTIMAL] and master_ip.SolCount > 0):\n",
        "\n",
        "        if master_ip.SolCount > 0: # Check if a solution was actually found\n",
        "            print(f\"Master IP Objective: {master_ip.ObjVal}\")\n",
        "            for idx, route_details in enumerate(final_routes):\n",
        "                route_id = route_details.get('id', f\"route_{idx}\")\n",
        "                if route_vars[route_id].X > 0.5: # Check if variable is selected\n",
        "                    selected_routes_details.append(route_details)\n",
        "            print(f\"Number of vehicles/routes used: {len(selected_routes_details)}\")\n",
        "        else: # e.g. Time limit hit before any feasible solution found\n",
        "            print(\"Master IP optimization ended, but no feasible integer solution found within limits.\")\n",
        "            return master_ip, None\n",
        "\n",
        "    elif master_ip.status == GRB.INFEASIBLE:\n",
        "        print(\"Master IP is INFEASIBLE. Check if all customers can be covered by the generated routes.\")\n",
        "        # You might want to compute an IIS here to debug\n",
        "        # master_ip.computeIIS()\n",
        "        # master_ip.write(\"master_ip_infeasible.ilp\")\n",
        "        return master_ip, None\n",
        "    else:\n",
        "        print(f\"Master IP did not solve to optimality or find a feasible solution. Status: {master_ip.status}\")\n",
        "        return master_ip, None\n",
        "\n",
        "    return master_ip, selected_routes_details\n",
        "\n",
        "\n",
        "def solve_espprc_with_multiple_time_windows(\n",
        "    customers_list_espprc: List[int],\n",
        "    demands_espprc: Dict[int, float],\n",
        "    service_times_espprc: Dict[int, float],\n",
        "    time_windows_data_espprc: Dict[int, List[Tuple[int, int]]],\n",
        "    coordinates_data_espprc: Dict[int, Tuple[float, float]], # Used if matrices not pre-calculated\n",
        "    vehicle_capacity_espprc: float,\n",
        "    speed_espprc: float, # Expected in dist/hr\n",
        "    alpha_penalty_espprc: float,\n",
        "    dual_prices_espprc: Dict[int, float],\n",
        "    depot_id_espprc: int = 0,\n",
        "    depot_return_time_limit_espprc: float = float('inf'),\n",
        "    # Optional pre-calculated matrices to save computation\n",
        "    travel_times_matrix: Optional[Dict[Tuple[int, int], float]] = None,\n",
        "    travel_distances_matrix: Optional[Dict[Tuple[int, int], float]] = None\n",
        ") -> Tuple[Optional[Dict[str, Any]], float]:\n",
        "    \"\"\"\n",
        "    Wrapper for ESPPRC solver.\n",
        "    Constructs the 'data' dictionary and calls the chosen ESPPRC algorithm.\n",
        "    Currently set to use forward labeling.\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate matrices if not provided\n",
        "    if travel_distances_matrix is None:\n",
        "        # Need calculate_distance_matrix function defined\n",
        "        travel_distances_matrix = calculate_distance_matrix(coordinates_data_espprc)\n",
        "    if travel_times_matrix is None:\n",
        "        # Need calculate_travel_time_matrix function defined\n",
        "        travel_times_matrix = calculate_travel_time_matrix(travel_distances_matrix, speed_espprc)\n",
        "\n",
        "    data_for_espprc = {\n",
        "        'customers_list': customers_list_espprc,\n",
        "        'demands': demands_espprc,\n",
        "        'service_times': service_times_espprc,\n",
        "        'time_windows_data': time_windows_data_espprc,\n",
        "        'coordinates_data': coordinates_data_espprc, # For reference, though matrices are used\n",
        "        'vehicle_capacity': vehicle_capacity_espprc,\n",
        "        'speed': speed_espprc, # dist/hr, used by calculate_travel_time_matrix if needed\n",
        "        'depot_id': depot_id_espprc,\n",
        "        'depot_info': {\n",
        "            'id': depot_id_espprc,\n",
        "            'time_window': (0, depot_return_time_limit_espprc), # ESPPRC needs depot's operational window end\n",
        "            'demand': 0.0,\n",
        "            'service_time': 0.0\n",
        "        },\n",
        "        # dual_prices are passed directly to the ESPPRC solver, not typically via this data dict\n",
        "        'travel_times': travel_times_matrix,\n",
        "        'travel_distances': travel_distances_matrix\n",
        "    }\n",
        "\n",
        "    # --- Choose ESPPRC Algorithm ---\n",
        "    # For now, using forward labeling as per previous setup.\n",
        "    # Replace with solve_espprc_bidirectional if/when it's fully implemented.\n",
        "    # Ensure the chosen ESPPRC function (e.g., solve_espprc_forward_labeling) is defined.\n",
        "\n",
        "    # Example: Assuming solve_espprc_forward_labeling is the one to use\n",
        "    # This function needs to be defined elsewhere in your script.\n",
        "    # from your_module import solve_espprc_forward_labeling\n",
        "\n",
        "    newly_found_routes = solve_espprc_forward_labeling(\n",
        "        data_for_espprc,\n",
        "        dual_prices_espprc,\n",
        "        alpha_penalty_espprc,\n",
        "        depot_return_time_limit_espprc # Pass the specific limit for depot return\n",
        "    )\n",
        "\n",
        "    if newly_found_routes:\n",
        "        # ESPPRC should return routes sorted by reduced cost, or we sort here.\n",
        "        # Assuming it returns a list of route dicts, each with 'reduced_cost'.\n",
        "        best_new_route = min(newly_found_routes, key=lambda x: x['reduced_cost'])\n",
        "        return best_new_route, best_new_route['reduced_cost']\n",
        "    else:\n",
        "        return None, float('inf')\n",
        "\n",
        "\n",
        "def column_generation_solver(initial_routes: List[Dict[str, Any]],\n",
        "                             customers_list: List[int],\n",
        "                             demands: Dict[int, float],\n",
        "                             service_times: Dict[int, float],\n",
        "                             time_windows_data: Dict[int, List[Tuple[int, int]]],\n",
        "                             coordinates_data: Dict[int, Tuple[float, float]],\n",
        "                             vehicle_capacity: float,\n",
        "                             speed: float,  # Expected in dist/hr\n",
        "                             alpha_penalty: float,\n",
        "                             max_cg_iterations: int = 50,\n",
        "                             # Added parameters for depot information:\n",
        "                             depot_id_cg: int = 1,\n",
        "                             depot_max_time_cg: float = 1440.0  # Default to 24 hours in minutes\n",
        "                            ) -> Tuple[Optional[gp.Model], Optional[List[Dict[str, Any]]], List[Dict[str, Any]]]:\n",
        "    \"\"\"\n",
        "    Main column generation loop.\n",
        "\n",
        "    Args:\n",
        "        ... (standard VRP data) ...\n",
        "        depot_id_cg: The ID of the depot node.\n",
        "        depot_max_time_cg: The latest time a vehicle can return to the depot (minutes from midnight).\n",
        "\n",
        "    Returns:\n",
        "        (final_ip_model, selected_routes_list, all_routes_in_pool_list)\n",
        "    \"\"\"\n",
        "    current_routes_pool = list(initial_routes)\n",
        "\n",
        "    # Pre-calculate travel times and distances once for the entire CG process\n",
        "    # These are based on the full coordinates_data and speed.\n",
        "    # Ensure these helper functions are defined in your environment.\n",
        "    print(\"CG: Pre-calculating distance and travel time matrices...\")\n",
        "    cg_travel_distances_matrix = calculate_distance_matrix(coordinates_data)\n",
        "    cg_travel_times_matrix = calculate_travel_time_matrix(cg_travel_distances_matrix, speed)\n",
        "    print(\"CG: Matrices calculated.\")\n",
        "\n",
        "    last_rmp_obj_val = float('inf')\n",
        "\n",
        "    for iteration in range(max_cg_iterations):\n",
        "        print(f\"\\n--- Column Generation Iteration: {iteration + 1} ---\")\n",
        "        print(f\"Current number of routes in RMP: {len(current_routes_pool)}\")\n",
        "\n",
        "        if not current_routes_pool:\n",
        "            print(\"Error in CG: Route pool is empty. Cannot solve RMP. This might happen if initial routes are all invalid.\")\n",
        "            # Attempt to generate at least one route if possible, e.g., for each customer if feasible\n",
        "            # This is a recovery mechanism, ideally initial_routes should be valid.\n",
        "            print(\"Attempting to generate basic single-customer routes for ESPPRC...\")\n",
        "            temp_dual_prices = {cust_id: 1000 for cust_id in customers_list} # High artificial duals\n",
        "\n",
        "            # Call ESPPRC with artificial duals to try to get *any* feasible route\n",
        "            # This call to ESPPRC is to bootstrap if the pool is empty.\n",
        "            # It might be better to ensure initial_routes are valid from the start.\n",
        "            bootstrap_route, bootstrap_rc = solve_espprc_with_multiple_time_windows(\n",
        "                customers_list_espprc=customers_list, demands_espprc=demands,\n",
        "                service_times_espprc=service_times, time_windows_data_espprc=time_windows_data,\n",
        "                coordinates_data_espprc=coordinates_data, vehicle_capacity_espprc=vehicle_capacity,\n",
        "                speed_espprc=speed, alpha_penalty_espprc=alpha_penalty,\n",
        "                dual_prices_espprc=temp_dual_prices, depot_id_espprc=depot_id_cg,\n",
        "                depot_return_time_limit_espprc=depot_max_time_cg,\n",
        "                travel_times_matrix=cg_travel_times_matrix,\n",
        "                travel_distances_matrix=cg_travel_distances_matrix\n",
        "            )\n",
        "            if bootstrap_route:\n",
        "                print(f\"Bootstrap: Found an initial route: {bootstrap_route['id']}\")\n",
        "                current_routes_pool.append(bootstrap_route)\n",
        "            else:\n",
        "                print(\"Bootstrap failed: Could not generate any initial route via ESPPRC.\")\n",
        "                return None, None, current_routes_pool\n",
        "\n",
        "\n",
        "        rmp_model, dual_prices = solve_restricted_master_problem(\n",
        "            current_routes_pool, customers_list, alpha_penalty\n",
        "        )\n",
        "\n",
        "        if rmp_model is None or dual_prices is None:\n",
        "            print(\"CG: Failed to solve RMP. Stopping column generation.\")\n",
        "            break\n",
        "\n",
        "        current_rmp_obj = rmp_model.ObjVal\n",
        "        print(f\"RMP Objective Value: {current_rmp_obj:.2f}\")\n",
        "        # print(f\"Dual prices from RMP: {dual_prices}\") # For debugging\n",
        "\n",
        "        # Solve the Pricing Subproblem (ESPPRC)\n",
        "        best_new_route, reduced_cost = solve_espprc_with_multiple_time_windows(\n",
        "            customers_list_espprc=customers_list,\n",
        "            demands_espprc=demands,\n",
        "            service_times_espprc=service_times,\n",
        "            time_windows_data_espprc=time_windows_data,\n",
        "            coordinates_data_espprc=coordinates_data, # Passed for completeness, matrices are used\n",
        "            vehicle_capacity_espprc=vehicle_capacity,\n",
        "            speed_espprc=speed, # dist/hr\n",
        "            alpha_penalty_espprc=alpha_penalty,\n",
        "            dual_prices_espprc=dual_prices,\n",
        "            depot_id_espprc=depot_id_cg, # Use the passed depot ID\n",
        "            depot_return_time_limit_espprc=depot_max_time_cg, # Use the passed depot max time\n",
        "            travel_times_matrix=cg_travel_times_matrix, # Pass pre-calculated matrix\n",
        "            travel_distances_matrix=cg_travel_distances_matrix # Pass pre-calculated matrix\n",
        "        )\n",
        "\n",
        "        if best_new_route and reduced_cost < -1e-6:  # Threshold for negative reduced cost\n",
        "            print(f\"Found new route with ID {best_new_route.get('id', 'N/A')}, reduced cost: {reduced_cost:.2f}\")\n",
        "            # print(f\"Route details: Customers {best_new_route['customers_served']}, Cost {best_new_route['cost']:.2f}\")\n",
        "\n",
        "            # Basic duplicate check (customers and cost)\n",
        "            is_duplicate = False\n",
        "            for r_in_pool in current_routes_pool:\n",
        "                if set(r_in_pool['customers_served']) == set(best_new_route['customers_served']) and \\\n",
        "                   abs(r_in_pool['cost'] - best_new_route['cost']) < 1e-6:\n",
        "                    is_duplicate = True\n",
        "                    # print(\"New route is considered a duplicate of an existing route in the pool.\")\n",
        "                    break\n",
        "\n",
        "            if not is_duplicate:\n",
        "                current_routes_pool.append(best_new_route)\n",
        "            else:\n",
        "                # If it's a duplicate but still has the most negative reduced cost,\n",
        "                # it might mean the LP is optimal or cycling.\n",
        "                # A more robust check for stopping might be if reduced_cost is very close to zero\n",
        "                # for several iterations or if RMP objective doesn't improve.\n",
        "                print(\"New route is a duplicate. If reduced cost is still significantly negative, consider alternative stopping criteria.\")\n",
        "                if reduced_cost > -0.01: # If not significantly negative, likely near optimal\n",
        "                    print(\"Reduced cost close to zero, stopping CG.\")\n",
        "                    break\n",
        "                # else continue, maybe it helps break cycling or explore other parts of solution space\n",
        "        else:\n",
        "            print(\"No new route with sufficiently negative reduced cost found. LP likely optimal for current columns.\")\n",
        "            break # Exit loop if pricing problem proves LP optimality for the current set of columns\n",
        "\n",
        "        # Optional: Check for RMP objective stalling\n",
        "        if abs(current_rmp_obj - last_rmp_obj_val) < 1e-4 and iteration > 5: # Arbitrary stall check\n",
        "            print(\"RMP objective has stalled for several iterations. Stopping CG.\")\n",
        "            break\n",
        "        last_rmp_obj_val = current_rmp_obj\n",
        "\n",
        "\n",
        "    print(\"\\n--- Column Generation Finished ---\")\n",
        "    print(f\"Final number of routes generated: {len(current_routes_pool)}\")\n",
        "\n",
        "    if not current_routes_pool:\n",
        "        print(\"CG Error: No routes in the pool to solve the Master IP.\")\n",
        "        return None, None, []\n",
        "\n",
        "    print(\"\\nSolving final Master Problem as IP...\")\n",
        "    final_ip_model, selected_routes = solve_master_ip(\n",
        "        current_routes_pool, customers_list, alpha_penalty\n",
        "    )\n",
        "\n",
        "    if selected_routes is not None: # Check if a solution (even if empty) was returned\n",
        "        # (Result processing happens in the main script)\n",
        "        return final_ip_model, selected_routes, current_routes_pool\n",
        "    else: # Master IP failed or returned None for selected_routes\n",
        "        print(\"No final integer solution found or Master IP failed.\")\n",
        "        return final_ip_model, None, current_routes_pool"
      ],
      "metadata": {
        "id": "5NpxgFAiMAsc"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def time_to_minutes(time_str):\n",
        "    \"\"\"Convert time string 'HH:MM' to minutes from midnight.\"\"\"\n",
        "    hours, minutes = map(int, time_str.split(':'))\n",
        "    return hours * 60 + minutes\n",
        "\n",
        "def load_coordinates(filepath):\n",
        "    \"\"\"Load coordinates from instance_coordinates.txt\"\"\"\n",
        "    coordinates = {}\n",
        "    with open(filepath, 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) == 3:\n",
        "                node_id = int(parts[0])\n",
        "                x = float(parts[1])\n",
        "                y = float(parts[2])\n",
        "                coordinates[node_id] = (x, y)\n",
        "    return coordinates\n",
        "\n",
        "def load_demands(filepath):\n",
        "    \"\"\"Load demands from instance_demand.txt\"\"\"\n",
        "    demands = {}\n",
        "    with open(filepath, 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) == 2:\n",
        "                customer_id = int(parts[0])\n",
        "                demand = float(parts[1])\n",
        "                demands[customer_id] = demand\n",
        "    return demands\n",
        "\n",
        "def load_service_times(filepath):\n",
        "    \"\"\"Load service times from instance_service_time.txt\"\"\"\n",
        "    service_times = {}\n",
        "    with open(filepath, 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) == 2:\n",
        "                customer_id = int(parts[0])\n",
        "                service_time = float(parts[1])  # in minutes\n",
        "                service_times[customer_id] = service_time\n",
        "    return service_times\n",
        "\n",
        "def load_time_windows(filepath):\n",
        "    \"\"\"Load time windows from instance_time_windows.txt\"\"\"\n",
        "    time_windows = {}\n",
        "    with open(filepath, 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) == 3:\n",
        "                customer_id = int(parts[0])\n",
        "                start_time = time_to_minutes(parts[1])\n",
        "                end_time = time_to_minutes(parts[2])\n",
        "\n",
        "                if customer_id not in time_windows:\n",
        "                    time_windows[customer_id] = []\n",
        "                time_windows[customer_id].append((start_time, end_time))\n",
        "\n",
        "    # Sort time windows for each customer\n",
        "    for customer_id in time_windows:\n",
        "        time_windows[customer_id].sort()\n",
        "\n",
        "    return time_windows\n",
        "\n",
        "def calculate_distance_matrix(coordinates):\n",
        "    \"\"\"Calculate Euclidean distances between all pairs of locations\"\"\"\n",
        "    distances = {}\n",
        "    for i in coordinates:\n",
        "        for j in coordinates:\n",
        "            if i != j:\n",
        "                x1, y1 = coordinates[i]\n",
        "                x2, y2 = coordinates[j]\n",
        "                dist = math.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
        "                distances[(i, j)] = dist\n",
        "            else:\n",
        "                distances[(i, j)] = 0.0\n",
        "    return distances\n",
        "\n",
        "def calculate_travel_time_matrix(distances, speed=1000):\n",
        "    \"\"\"Calculate travel times from distances and speed\"\"\"\n",
        "    travel_times = {}\n",
        "    for (i, j), distance in distances.items():\n",
        "        if i != j:\n",
        "            # Add 1 for triangle inequality as mentioned in the paper\n",
        "            travel_time = distance / speed  # in hours\n",
        "            travel_times[(i, j)] = travel_time * 60  # convert to minutes\n",
        "        else:\n",
        "            travel_times[(i, j)] = 0.0\n",
        "    return travel_times\n",
        "\n",
        "def setup_depot_info(depot_id=1, planning_horizon_hours=24):\n",
        "    \"\"\"Setup depot information\"\"\"\n",
        "    return {\n",
        "        'id': depot_id,\n",
        "        'time_window': (0, planning_horizon_hours * 60),  # 24 hours in minutes\n",
        "        'demand': 0,\n",
        "        'service_time': 0\n",
        "    }\n",
        "\n",
        "def load_and_preprocess_data(base_path, instance_name, speed=1000, vehicle_capacity=12600, depot_id=1):\n",
        "    \"\"\"\n",
        "    Load all instance data and perform preprocessing for VRPMTW.\n",
        "\n",
        "    Args:\n",
        "        base_path (str): Path to directory containing instance files\n",
        "        instance_name (str): Name prefix of instance files (e.g., 'instance' for instance_*.txt)\n",
        "        speed (float): Vehicle speed in distance units per hour (default: 1000)\n",
        "        vehicle_capacity (float): Vehicle capacity in kg (default: 12600)\n",
        "        depot_id (int): ID of the depot node (default: 1)\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing all preprocessed data with the following keys:\n",
        "            - 'customers_list' (list): List of customer IDs (excludes depot)\n",
        "            - 'coordinates_data' (dict): {node_id: (x, y)} coordinates for all nodes\n",
        "            - 'demands' (dict): {customer_id: demand_kg} customer demands in kg\n",
        "            - 'service_times' (dict): {customer_id: service_time_minutes} service times\n",
        "            - 'time_windows_data' (dict): {customer_id: [(start_min, end_min), ...]}\n",
        "                                         multiple time windows per customer in minutes from midnight\n",
        "            - 'travel_times' (dict): {(i,j): travel_time_minutes} travel times between all node pairs\n",
        "            - 'travel_distances' (dict): {(i,j): euclidean_distance} distances between all node pairs\n",
        "            - 'depot_info' (dict): {'id': depot_id, 'time_window': (0, 1440), 'demand': 0, 'service_time': 0}\n",
        "            - 'vehicle_capacity' (float): Vehicle capacity in kg\n",
        "            - 'speed' (float): Vehicle speed in distance units per hour\n",
        "            - 'depot_id' (int): ID of the depot node\n",
        "\n",
        "    Example:\n",
        "        data = load_and_preprocess_data(\"/content\", \"instance\")\n",
        "        customers = data['customers_list']  # [2, 3, 4, 5, ...]\n",
        "        demand_customer_2 = data['demands'][2]  # 2290.0\n",
        "        travel_time_1_to_2 = data['travel_times'][(1, 2)]  # 15.4 minutes\n",
        "    \"\"\"\n",
        "    print(\"Loading instance data...\")\n",
        "\n",
        "    # Construct file paths\n",
        "    coords_file = f\"{base_path}/{instance_name}_coordinates.txt\"\n",
        "    demand_file = f\"{base_path}/{instance_name}_demand.txt\"\n",
        "    service_file = f\"{base_path}/{instance_name}_service_time.txt\"\n",
        "    tw_file = f\"{base_path}/{instance_name}_time_windows.txt\"\n",
        "\n",
        "    # Load raw data\n",
        "    coordinates = load_coordinates(coords_file)\n",
        "    demands = load_demands(demand_file)\n",
        "    service_times = load_service_times(service_file)\n",
        "    time_windows = load_time_windows(tw_file)\n",
        "\n",
        "    # Set depot service time to 0 if not specified\n",
        "    if depot_id not in service_times:\n",
        "        service_times[depot_id] = 0\n",
        "\n",
        "    # Set depot time window (24 hours)\n",
        "    time_windows[depot_id] = [(0, 24 * 60)]  # 0 to 1440 minutes\n",
        "\n",
        "    # Calculate distance and travel time matrices\n",
        "    print(\"Calculating distance and travel time matrices...\")\n",
        "    distances = calculate_distance_matrix(coordinates)\n",
        "    travel_times = calculate_travel_time_matrix(distances, speed)\n",
        "\n",
        "    # Create customer list (exclude depot)\n",
        "    customers_list = [node_id for node_id in coordinates.keys() if node_id != depot_id]\n",
        "\n",
        "    # Setup depot info\n",
        "    depot_info = setup_depot_info(depot_id)\n",
        "\n",
        "    print(f\"Loaded {len(customers_list)} customers\")\n",
        "    print(f\"Depot: {depot_id}\")\n",
        "    print(f\"Vehicle capacity: {vehicle_capacity} kg\")\n",
        "    print(f\"Speed: {speed} distance units/hour\")\n",
        "\n",
        "    return {\n",
        "        'customers_list': customers_list,\n",
        "        'coordinates_data': coordinates,\n",
        "        'demands': demands,\n",
        "        'service_times': service_times,\n",
        "        'time_windows_data': time_windows,\n",
        "        'travel_times': travel_times,\n",
        "        'travel_distances': distances,\n",
        "        'depot_info': depot_info,\n",
        "        'vehicle_capacity': vehicle_capacity,\n",
        "        'speed': speed,\n",
        "        'depot_id': depot_id\n",
        "    }\n",
        "\n",
        "# Test function to validate data loading\n",
        "def print_data_summary(data):\n",
        "    \"\"\"Print a summary of loaded data for validation\"\"\"\n",
        "    print(\"\\n=== DATA SUMMARY ===\")\n",
        "    print(f\"Number of customers: {len(data['customers_list'])}\")\n",
        "    print(f\"Depot ID: {data['depot_info']['id']}\")\n",
        "    print(f\"Vehicle capacity: {data['vehicle_capacity']} kg\")\n",
        "\n",
        "    # Sample customer info\n",
        "    sample_customer = data['customers_list'][0] if data['customers_list'] else None\n",
        "    if sample_customer:\n",
        "        print(f\"\\nSample customer {sample_customer}:\")\n",
        "        print(f\"  Coordinates: {data['coordinates_data'][sample_customer]}\")\n",
        "        print(f\"  Demand: {data['demands'][sample_customer]} kg\")\n",
        "        print(f\"  Service time: {data['service_times'].get(sample_customer, 'N/A')} min\")\n",
        "        print(f\"  Time windows: {data['time_windows_data'].get(sample_customer, 'N/A')}\")\n",
        "\n",
        "    # Check for customers with multiple time windows\n",
        "    multi_tw_customers = []\n",
        "    for cust in data['customers_list']:\n",
        "        if cust in data['time_windows_data'] and len(data['time_windows_data'][cust]) > 1:\n",
        "            multi_tw_customers.append(cust)\n",
        "\n",
        "    if multi_tw_customers:\n",
        "        print(f\"\\nCustomers with multiple time windows: {multi_tw_customers}\")\n",
        "        for cust in multi_tw_customers[:3]:  # Show first 3\n",
        "            print(f\"  Customer {cust}: {data['time_windows_data'][cust]}\")\n",
        "\n",
        "    print(\"===================\\n\")"
      ],
      "metadata": {
        "id": "hJv9oGB1MJud"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_route(route_sequence, departure_time, data):\n",
        "    \"\"\"\n",
        "    Evaluate a potential route for feasibility and calculate total travel distance.\n",
        "\n",
        "    Service can span multiple time windows: if service time extends beyond the current\n",
        "    window, the driver pauses and waits for the next window to resume service.\n",
        "\n",
        "    Args:\n",
        "        route_sequence (list): Ordered list of customer IDs to visit\n",
        "        departure_time (float): Departure time from depot in minutes from midnight\n",
        "        data (dict): Preprocessed data dictionary\n",
        "\n",
        "    Returns:\n",
        "        float or None: Total travel distance if feasible, None if infeasible\n",
        "    \"\"\"\n",
        "    if not route_sequence:\n",
        "        return 0.0  # Empty route is valid with zero distance\n",
        "\n",
        "    depot_id = data['depot_id']\n",
        "    vehicle_capacity = data['vehicle_capacity']\n",
        "    travel_times = data['travel_times']\n",
        "    travel_distances = data['travel_distances']\n",
        "    demands = data['demands']\n",
        "    service_times = data['service_times']\n",
        "    time_windows_data = data['time_windows_data']\n",
        "    depot_time_window = data['depot_info']['time_window']\n",
        "\n",
        "    # Initialize route state\n",
        "    current_time = departure_time\n",
        "    current_load = 0.0\n",
        "    current_location = depot_id\n",
        "    total_distance = 0.0\n",
        "\n",
        "    # Check if departure time is within depot's operational window\n",
        "    if current_time < depot_time_window[0] or current_time > depot_time_window[1]:\n",
        "        return None\n",
        "\n",
        "    # Visit each customer in sequence\n",
        "    for customer_id in route_sequence:\n",
        "        # Check capacity constraint\n",
        "        customer_demand = demands[customer_id]\n",
        "        if current_load + customer_demand > vehicle_capacity:\n",
        "            return None  # Capacity exceeded\n",
        "\n",
        "        # Calculate travel to customer\n",
        "        travel_time = travel_times[(current_location, customer_id)]\n",
        "        travel_distance = travel_distances[(current_location, customer_id)]\n",
        "        arrival_time = current_time + travel_time\n",
        "\n",
        "        # Get customer's time windows and service time\n",
        "        customer_time_windows = time_windows_data[customer_id]\n",
        "        total_service_time = service_times[customer_id]\n",
        "\n",
        "        # Sort time windows by start time\n",
        "        sorted_windows = sorted(customer_time_windows, key=lambda x: x[0])\n",
        "\n",
        "        # Find first window where service can start (arrival_time <= window_end)\n",
        "        service_start_window_idx = None\n",
        "        for i, (window_start, window_end) in enumerate(sorted_windows):\n",
        "            if arrival_time <= window_end:\n",
        "                service_start_window_idx = i\n",
        "                break\n",
        "\n",
        "        if service_start_window_idx is None:\n",
        "            return None  # Cannot start service in any window\n",
        "\n",
        "        # Simulate service across potentially multiple windows\n",
        "        remaining_service_time = total_service_time\n",
        "        window_idx = service_start_window_idx\n",
        "        current_service_time = arrival_time\n",
        "\n",
        "        while remaining_service_time > 0:\n",
        "            if window_idx >= len(sorted_windows):\n",
        "                return None  # No more windows available\n",
        "\n",
        "            window_start, window_end = sorted_windows[window_idx]\n",
        "\n",
        "            # Determine when service can actually start in this window\n",
        "            service_start_in_window = max(current_service_time, window_start)\n",
        "\n",
        "            # If we've already passed this window, move to next\n",
        "            if service_start_in_window >= window_end:\n",
        "                window_idx += 1\n",
        "                continue\n",
        "\n",
        "            # Calculate how much service can be done in this window\n",
        "            available_time_in_window = window_end - service_start_in_window\n",
        "            service_in_this_window = min(remaining_service_time, available_time_in_window)\n",
        "\n",
        "            # Update service progress\n",
        "            remaining_service_time -= service_in_this_window\n",
        "            current_service_time = service_start_in_window + service_in_this_window\n",
        "\n",
        "            # If service is complete, break\n",
        "            if remaining_service_time <= 0:\n",
        "                break\n",
        "\n",
        "            # Need to continue in next window - jump to next window start\n",
        "            window_idx += 1\n",
        "            if window_idx < len(sorted_windows):\n",
        "                current_service_time = sorted_windows[window_idx][0]\n",
        "\n",
        "        if remaining_service_time > 0:\n",
        "            return None  # Service could not be completed within available windows\n",
        "\n",
        "        # Update route state\n",
        "        current_time = current_service_time\n",
        "        current_load += customer_demand\n",
        "        current_location = customer_id\n",
        "        total_distance += travel_distance\n",
        "\n",
        "    # Check if can return to depot within depot's time window\n",
        "    if current_location != depot_id:  # Need to return to depot\n",
        "        return_travel_time = travel_times[(current_location, depot_id)]\n",
        "        return_distance = travel_distances[(current_location, depot_id)]\n",
        "        depot_arrival_time = current_time + return_travel_time\n",
        "\n",
        "        if depot_arrival_time > depot_time_window[1]:\n",
        "            return None  # Cannot return to depot in time\n",
        "\n",
        "        total_distance += return_distance\n",
        "\n",
        "    return total_distance"
      ],
      "metadata": {
        "id": "Iheu1tKUMMAe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Label:\n",
        "    \"\"\"\n",
        "    Represents a label for a partial path in the ESPPRC (both forward and backward).\n",
        "\n",
        "    Attributes:\n",
        "        current_node_id (int): The last/first customer visited (forward/backward).\n",
        "        visited_customers_set (set): Set of customer IDs already included in this partial path.\n",
        "        current_load (float): Total demand accumulated so far.\n",
        "        dominant_intervals (list): List of tuples (earliest_start, latest_start, accumulated_distance).\n",
        "            For forward: dominant forward start intervals at current_node_id\n",
        "            For backward: dominant backward start intervals at current_node_id\n",
        "        accumulated_dual_value_sum (float): Sum of dual variables (pi_i) of visited customers.\n",
        "        path_sequence (list): Ordered list of customer IDs visited in this partial path.\n",
        "        is_forward (bool): True for forward label, False for backward label.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 current_node_id: int,\n",
        "                 visited_customers_set: set,\n",
        "                 current_load: float,\n",
        "                 dominant_forward_start_intervals: list,\n",
        "                 accumulated_dual_value_sum: float,\n",
        "                 path_sequence: list,\n",
        "                 is_forward: bool = True):\n",
        "        self.current_node_id = current_node_id\n",
        "        self.visited_customers_set = set(visited_customers_set)\n",
        "        self.current_load = current_load\n",
        "        self.dominant_forward_start_intervals = sorted(dominant_forward_start_intervals, key=lambda x: x[0])\n",
        "        self.accumulated_dual_value_sum = accumulated_dual_value_sum\n",
        "        self.path_sequence = list(path_sequence)\n",
        "        self.is_forward = is_forward\n",
        "\n",
        "    def get_earliest_start_of_first_interval(self) -> float:\n",
        "        \"\"\"Returns E_Lf(1) for forward or E_Lb(1) for backward labels.\"\"\"\n",
        "        if not self.dominant_forward_start_intervals:\n",
        "            return float('inf')\n",
        "        return self.dominant_forward_start_intervals[0][0]\n",
        "\n",
        "    def get_latest_start_of_last_interval(self) -> float:\n",
        "        \"\"\"Returns L_Lf(|F_Lf|) for forward or L_Lb(|B_Lb|) for backward labels.\"\"\"\n",
        "        if not self.dominant_forward_start_intervals:\n",
        "            return -float('inf')\n",
        "        return self.dominant_forward_start_intervals[-1][1]\n",
        "\n",
        "    def get_num_intervals(self) -> int:\n",
        "        \"\"\"Returns number of dominant intervals.\"\"\"\n",
        "        return len(self.dominant_forward_start_intervals)\n",
        "\n",
        "    def __repr__(self):\n",
        "        direction = \"Forward\" if self.is_forward else \"Backward\"\n",
        "        return (f\"{direction}Label(Node: {self.current_node_id}, \"\n",
        "                f\"Load: {self.current_load:.2f}, \"\n",
        "                f\"Path: {self.path_sequence}, \"\n",
        "                f\"#Intervals: {self.get_num_intervals()}, \"\n",
        "                f\"DualSum: {self.accumulated_dual_value_sum:.2f})\")"
      ],
      "metadata": {
        "id": "D5h9bZHuMN-3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# Assuming the Label class from Step 3 is defined above or imported\n",
        "# class Label:\n",
        "#     ... (implementation from Step 3) ...\n",
        "\n",
        "def initialize_depot_label(data: dict) -> Label:\n",
        "    \"\"\"\n",
        "    Creates the initial Label object for the depot.\n",
        "\n",
        "    Args:\n",
        "        data (dict): The preprocessed data dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Label: The initial label starting at the depot.\n",
        "    \"\"\"\n",
        "    depot_id = data['depot_info']['id']\n",
        "    depot_time_window = data['depot_info']['time_window'] # (start_time, end_time)\n",
        "    initial_intervals = [(depot_time_window[0], depot_time_window[1], 0.0)]\n",
        "\n",
        "    return Label(\n",
        "        current_node_id=depot_id,\n",
        "        visited_customers_set=set(),\n",
        "        current_load=0.0,\n",
        "        dominant_forward_start_intervals=initial_intervals,\n",
        "        accumulated_dual_value_sum=0.0,\n",
        "        path_sequence=[depot_id]\n",
        "    )\n",
        "\n",
        "def _calculate_tau_ij_from_paper(node_i_id: int, node_j_id: int, data: dict) -> float:\n",
        "    \"\"\"\n",
        "    Calculates tau_ij as used in the paper: service time at node i + travel time from i to j.\n",
        "    Note: The paper's tau_ij definition on page 43 \"Duration tau_ij includes the service\n",
        "    time at node i and the travel time between nodes i and j.\"\n",
        "    My previous _calculate_tau_ij was identical.\n",
        "    \"\"\"\n",
        "    service_time_at_i = 0.0\n",
        "    if node_i_id != data['depot_info']['id']: # If node i is not the depot\n",
        "        service_time_at_i = data['service_times'].get(node_i_id, 0.0)\n",
        "    else: # node_i_id is the depot\n",
        "        service_time_at_i = data['depot_info']['service_time'] # This should be 0\n",
        "\n",
        "    travel_time_ij = data['travel_times'].get((node_i_id, node_j_id), float('inf'))\n",
        "    return service_time_at_i + travel_time_ij\n",
        "\n",
        "def _generate_candidate_intervals_at_j(label_at_i: Label, customer_j_id: int, data: dict) -> list:\n",
        "    \"\"\"\n",
        "    Generates candidate forward start intervals at customer_j_id by extending label_at_i,\n",
        "    based on Algorithm C.1 from the appendix[cite: 77, 79].\n",
        "    The 'cost' associated with intervals will be accumulated travel distance.\n",
        "    \"\"\"\n",
        "    node_i_id = label_at_i.current_node_id\n",
        "    generated_intervals = []\n",
        "\n",
        "    # tau_v(Lf)j in Algorithm C.1 is service_at_v(Lf) + travel_v(Lf)_to_j\n",
        "    tau_node_i_j = _calculate_tau_ij_from_paper(node_i_id, customer_j_id, data)\n",
        "    travel_dist_ij = data['travel_distances'].get((node_i_id, customer_j_id), float('inf'))\n",
        "\n",
        "    original_time_windows_at_j = data['time_windows_data'].get(customer_j_id, [])\n",
        "    dominant_intervals_at_i = label_at_i.dominant_forward_start_intervals\n",
        "\n",
        "    if node_i_id == data['depot_info']['id']: # Case: v(Lf) = 0 (depot) [cite: 77]\n",
        "        # Lines 2-5 of Algorithm C.1 [cite: 77]\n",
        "        # tau_0j for duration. For distance, it's data['travel_distances'][(depot_id, customer_j_id)]\n",
        "        depot_travel_dist_to_j = data['travel_distances'].get((data['depot_info']['id'], customer_j_id), float('inf'))\n",
        "        for e_j_t, l_j_t in original_time_windows_at_j:\n",
        "            # Check direct reachability from depot based on depot's end time and tau_0j\n",
        "            # Depot earliest departure is dominant_intervals_at_i[0][0] (usually 0)\n",
        "            # Depot latest departure is dominant_intervals_at_i[0][1]\n",
        "            depot_earliest_departure = dominant_intervals_at_i[0][0] # E_Lf(y) where y is the single depot interval\n",
        "\n",
        "            # Effective earliest start at j if departing depot at earliest time\n",
        "            earliest_service_start_at_j = max(depot_earliest_departure + tau_node_i_j, e_j_t)\n",
        "\n",
        "            # Effective latest start at j if departing depot at latest time\n",
        "            latest_service_start_at_j = min(max(dominant_intervals_at_i[0][1] + tau_node_i_j, e_j_t), l_j_t)\n",
        "\n",
        "            if earliest_service_start_at_j <= latest_service_start_at_j :\n",
        "                 generated_intervals.append(\n",
        "                    (earliest_service_start_at_j, latest_service_start_at_j, depot_travel_dist_to_j)\n",
        "                )\n",
        "    else: # Case: v(Lf) != 0 (previous node is a customer) [cite: 77, 79]\n",
        "        # Lines 7-21 of Algorithm C.1 [cite: 77, 79]\n",
        "        for idx_y, y_interval in enumerate(dominant_intervals_at_i):\n",
        "            E_Lf_y, L_Lf_y, D_Lf_y = y_interval # D_Lf_y is acc_dist_to_i\n",
        "\n",
        "            for t_original_tw_j in original_time_windows_at_j:\n",
        "                e_j_t, l_j_t = t_original_tw_j\n",
        "\n",
        "                # Subcase: Interval lies before time window (Lines 9-13 of Alg C.1 [cite: 77, 79])\n",
        "                if L_Lf_y + tau_node_i_j < e_j_t:\n",
        "                    # Check dominance condition from Line 10 of Alg C.1 [cite: 77]\n",
        "                    generates_waiting_interval = False\n",
        "                    if idx_y == len(dominant_intervals_at_i) - 1: # y is the last interval F_Lf\n",
        "                        generates_waiting_interval = True\n",
        "                    else:\n",
        "                        E_Lf_y_plus_1 = dominant_intervals_at_i[idx_y+1][0]\n",
        "                        if E_Lf_y_plus_1 + tau_node_i_j > e_j_t:\n",
        "                            generates_waiting_interval = True\n",
        "\n",
        "                    if generates_waiting_interval:\n",
        "                        # Add [e_j^t, e_j^t] to intervals\n",
        "                        # Cost: D_Lf_y + travel_dist_ij. Waiting doesn't add to travel distance.\n",
        "                        # Algorithm C.1 adds d_Lf(y) + e_j^t - L_Lf(y) to g(Lf'). This reflects waiting time in duration.\n",
        "                        # For travel distance, it remains D_Lf_y + travel_dist_ij.\n",
        "                        generated_intervals.append((e_j_t, e_j_t, D_Lf_y + travel_dist_ij))\n",
        "\n",
        "                # Subcase: Overlap interval and time window (Lines 14-17 of Alg C.1 [cite: 77, 79])\n",
        "                # Condition: E_Lf(y) + tau_node_i_j <= l_j_t (combined with not being strictly before)\n",
        "                elif E_Lf_y + tau_node_i_j <= l_j_t: # This is the general overlap condition\n",
        "                    new_E_j = max(E_Lf_y + tau_node_i_j, e_j_t)\n",
        "                    new_L_j = min(L_Lf_y + tau_node_i_j, l_j_t) # Error in Alg C.1 line 15: uses min{max{L_Lf(y)...}}. Eq 3.5 is min{max{L_p(y)+tau, e_j^t}, l_j^t} for L_p'(z). Let's use 3.5 logic.\n",
        "                    # Re-evaluating L_p_prime_z from eq 3.5:\n",
        "                    new_L_j_corrected = min(max(L_Lf_y + tau_node_i_j, e_j_t), l_j_t)\n",
        "\n",
        "\n",
        "                    if new_E_j <= new_L_j_corrected: # Valid interval\n",
        "                        # Cost: D_Lf_y + travel_dist_ij. Alg C.1 (line 16) adds d_Lf(y) + tau_v(Lf),j.\n",
        "                        generated_intervals.append((new_E_j, new_L_j_corrected, D_Lf_y + travel_dist_ij))\n",
        "\n",
        "                # Break condition from Lines 18-20 of Alg C.1 [cite: 79]\n",
        "                if L_Lf_y + tau_node_i_j < l_j_t: # This is L_Lf(y) in Alg C.1, not L_p_prime_z.\n",
        "                                                 # If latest start from i + combined_time is before current TW end,\n",
        "                                                 # it means it cannot reach later original TWs of j *any earlier*.\n",
        "                                                 # This break is from the inner loop over T_j.\n",
        "                    # The comment in paper [cite: 86] states \"start interval y does not overlap with time windows t' > t\"\n",
        "                    # This implies that if L_Lf(y) + tau_v(Lf)j < l_j^t, we can break from iterating T_j FOR THE CURRENT y.\n",
        "                    # And go to the next y. The code in C.1 does break.\n",
        "                    pass # Python for loop will naturally go to next t_original_tw_j\n",
        "                         # The break in C.1 (line 19) is for the loop over t in Tj\n",
        "                         # This break should make it go to the next y interval if L_Lf(y) + tau < l_j^t\n",
        "                         # This logic is subtle. The algorithm says \"break\", which exits the innermost loop (over T_j).\n",
        "                         # So for the current 'y', if it cannot even reach the end of 'l_j_t', it surely cannot reach e_j^{t+1}.\n",
        "                         # This means for the current y, we are done with time windows of j.\n",
        "                    # Correction: The break should be for the inner loop over `t_original_tw_j`.\n",
        "                    # If true, no need to check later time windows of j FOR THE CURRENT y_interval.\n",
        "                    if L_Lf_y + tau_node_i_j < e_j_t: # If latest arrival from y is even before start of current TW_j\n",
        "                                                      # (and didn't form a waiting interval)\n",
        "                                                      # then it surely won't reach later TWs of j.\n",
        "                        break # This break corresponds to line 19 in Alg C.1, exiting T_j loop for current y.\n",
        "\n",
        "\n",
        "    return generated_intervals\n",
        "\n",
        "def _filter_dominant_intervals_pareto(candidate_intervals: list) -> list:\n",
        "    \"\"\"\n",
        "    Filters a list of candidate (E, L, D) intervals to keep only Pareto-dominant ones.\n",
        "    An interval I1=(E1, L1, D1) dominates I2=(E2, L2, D2) if E1<=E2, L1>=L2, D1<=D2\n",
        "    (and I1 != I2).\n",
        "    The resulting list is sorted by E, then -L, then D.\n",
        "    This function aims to produce a set that satisfies the non-overlapping property $L_k < E_{k+1}$\n",
        "    implicitly through strong dominance, or by a final merging step (simplified here).\n",
        "    \"\"\"\n",
        "    if not candidate_intervals:\n",
        "        return []\n",
        "\n",
        "    # Remove exact duplicates and sort for stable processing\n",
        "    # Sort by E (asc), then L (desc for wider), then D (asc for cheaper)\n",
        "    # This sorting helps in identifying dominated intervals more easily.\n",
        "    unique_sorted_candidates = sorted(list(set(candidate_intervals)), key=lambda x: (x[0], -x[1], x[2]))\n",
        "\n",
        "    dominant_intervals = []\n",
        "    for current_interval in unique_sorted_candidates:\n",
        "        is_dominated_by_existing = False\n",
        "        # Check if current_interval is dominated by any interval already in dominant_intervals\n",
        "        for existing_dominant_interval in dominant_intervals:\n",
        "            # E_exist <= E_curr, L_exist >= L_curr, D_exist <= D_curr\n",
        "            if (existing_dominant_interval[0] <= current_interval[0] and\n",
        "                existing_dominant_interval[1] >= current_interval[1] and\n",
        "                existing_dominant_interval[2] <= current_interval[2]):\n",
        "                if existing_dominant_interval != current_interval: # Strictly dominated or identical covered one\n",
        "                    is_dominated_by_existing = True\n",
        "                    break\n",
        "\n",
        "        if not is_dominated_by_existing:\n",
        "            # Remove any intervals from dominant_intervals that are now dominated by current_interval\n",
        "            new_dominant_list = []\n",
        "            for i in range(len(dominant_intervals)):\n",
        "                # Check if dominant_intervals[i] is dominated by current_interval\n",
        "                # E_curr <= E_exist, L_curr >= L_exist, D_curr <= D_exist\n",
        "                if not (current_interval[0] <= dominant_intervals[i][0] and\n",
        "                        current_interval[1] >= dominant_intervals[i][1] and\n",
        "                        current_interval[2] <= dominant_intervals[i][2] and\n",
        "                        current_interval != dominant_intervals[i]):\n",
        "                    new_dominant_list.append(dominant_intervals[i])\n",
        "            dominant_intervals = new_dominant_list\n",
        "            dominant_intervals.append(current_interval)\n",
        "            # Re-sort is important if adding changes order for next iteration's checks,\n",
        "            # but since unique_sorted_candidates is processed in order, this might be okay.\n",
        "            # For safety, sort at the end.\n",
        "            dominant_intervals.sort(key=lambda x: (x[0], -x[1], x[2]))\n",
        "\n",
        "\n",
        "    # The list dominant_intervals now contains Pareto-optimal intervals.\n",
        "    # To ensure strict non-overlapping $L_k < E_{k+1}$ as per Lemma 2.1[cite: 54]:\n",
        "    # This might require a more sophisticated merging/selection if Pareto set still has overlaps.\n",
        "    # For now, we return the sorted Pareto set. If overlaps cause issues later,\n",
        "    # this part (non-overlapping merging) needs to be implemented robustly.\n",
        "    # A simple greedy merge for same-cost adjacent/overlapping intervals:\n",
        "    if not dominant_intervals:\n",
        "        return []\n",
        "\n",
        "    final_merged_intervals = []\n",
        "    final_merged_intervals.append(dominant_intervals[0])\n",
        "\n",
        "    for i in range(1, len(dominant_intervals)):\n",
        "        curr_E, curr_L, curr_D = dominant_intervals[i]\n",
        "        last_E, last_L, last_D = final_merged_intervals[-1]\n",
        "\n",
        "        # If current interval is identical or strictly dominated by last_merged, it would have been filtered by Pareto.\n",
        "        # Check for merging opportunity: if current starts at or before last ends, AND same cost, and extends last.\n",
        "        if curr_E <= last_L + 1e-6 and curr_D == last_D : # Overlap or contiguous with same cost\n",
        "            # Merge: update L of the last interval in final_merged_intervals\n",
        "            final_merged_intervals[-1] = (last_E, max(last_L, curr_L), last_D)\n",
        "        elif curr_E > last_L: # No overlap, current starts after last ended\n",
        "            final_merged_intervals.append((curr_E, curr_L, curr_D))\n",
        "        else: # Overlap with different costs, or other complex cases.\n",
        "              # The Pareto filter should mean D_curr is not worse if it overlaps significantly.\n",
        "              # If curr_D < last_D and they overlap, Pareto should have handled it.\n",
        "              # This case means they are non-dominated but overlap.\n",
        "              # Example: last=(0,10,D=5), curr=(5,15,D=5) -> merged to (0,15,D=5)\n",
        "              # Example: last=(0,10,D=5), curr=(5,15,D=4) -> Pareto should keep (5,15,D=4) and (0,4.99,D=5)\n",
        "              # The current simplified merge above only handles same-cost overlaps.\n",
        "              # For now, if not same-cost mergeable and not starting after, just add if distinct.\n",
        "              # This part is the most heuristic if not following a specific algorithm from [forthcoming].\n",
        "              # Let's assume for now that after Pareto, if they are not same-cost mergeable, they form distinct dominant intervals.\n",
        "              # The proof of Lemma 2.1 implies dominance sorts this out.\n",
        "            if (curr_E, curr_L, curr_D) != final_merged_intervals[-1]: # Add if truly new after non-merge\n",
        "                 #This could still lead to overlaps if D values are different.\n",
        "                 #Safest is to return the pure Pareto set, sorted.\n",
        "                 #The problem statement \"dominant forward start intervals are non-overlapping\" is a strong one.\n",
        "                 #This means the filtering process MUST achieve this.\n",
        "                 #A more robust way:\n",
        "                 # If curr_E <= last_L (overlap):\n",
        "                 #    If curr_D < last_D:\n",
        "                 #        final_merged_intervals[-1] = (last_E, curr_E - epsilon, last_D) // Truncate last\n",
        "                 #        if final_merged_intervals[-1][0] > final_merged_intervals[-1][1]: final_merged_intervals.pop()\n",
        "                 #        final_merged_intervals.append((curr_E, curr_L, curr_D)) // Add current\n",
        "                 #    Else if curr_D == last_D: (handled by merge above)\n",
        "                 #    Else (curr_D > last_D): // Current is worse over the overlap\n",
        "                 #        temp_E = last_L + epsilon\n",
        "                 #        if temp_E <= curr_L:\n",
        "                 #            final_merged_intervals.append((temp_E, curr_L, curr_D)) // Add truncated current\n",
        "                 # else: (no overlap)\n",
        "                 #    final_merged_intervals.append((curr_E, curr_L, curr_D))\n",
        "\n",
        "                # Returning just the Pareto set and relying on later dominance (Prop 3.1) to resolve.\n",
        "                # For now, let's stick to the simpler merge for same-cost, and add if distinct non-overlapping.\n",
        "                # The code above already did the simple merge. If it didn't merge and wasn't disjoint, it's an issue.\n",
        "                # Let's return the sorted Pareto list as `dominant_intervals`. The non-overlapping property is hard to guarantee\n",
        "                # perfectly without the exact procedure from Hoogeboom et al. [forthcoming].\n",
        "                # The provided Lemma 2.1 proof uses Prop 2.1 (from Chapter 2, for duration) to argue that\n",
        "                # if intervals are not properly ordered (L_q < E_{q+1}), one must dominate the other.\n",
        "                # This means our Pareto filter (E1<=E2, L1>=L2, D1<=D2) is the key.\n",
        "                pass # The dominant_intervals list is already sorted from Pareto step.\n",
        "\n",
        "    # Return the result of Pareto dominance, sorted.\n",
        "    return dominant_intervals\n",
        "\n",
        "\n",
        "def extend_label_to_new_node(label_at_i: Label, next_node_id: int, data: dict) -> list:\n",
        "    \"\"\"\n",
        "    Wrapper function for extending a label from node i to next_node_id (customer or depot).\n",
        "    If next_node_id is a customer, uses _generate_candidate_intervals_at_j.\n",
        "    If next_node_id is the depot, no new intervals are generated, but feasibility is checked.\n",
        "\n",
        "    Args:\n",
        "        label_at_i (Label): The label ending at node i.\n",
        "        next_node_id (int): The ID of the next node to visit.\n",
        "        data (dict): The preprocessed data dictionary.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of new dominant forward start intervals for the path extended to next_node_id.\n",
        "              Returns empty list if no feasible extension or if next_node_id is depot (as intervals\n",
        "              are properties of customer service starts).\n",
        "    \"\"\"\n",
        "    if next_node_id == data['depot_info']['id']: # Extending back to depot\n",
        "        # No new \"forward start intervals\" are typically defined for the end depot.\n",
        "        # Feasibility of reaching end depot is checked when forming a full route.\n",
        "        # The \"dominant_forward_start_intervals\" attribute is for service start at a customer.\n",
        "        # The cost to reach the end depot is simply D_i + travel_dist(i, depot_end).\n",
        "        # The \"time\" for the label reaching end depot will be the path completion time.\n",
        "        return [] # Or perhaps special handling if labels are to store completion times at depot.\n",
        "                  # For now, consistent with paper, these intervals are for customer service.\n",
        "    else: # Extending to another customer\n",
        "        candidate_intervals = _generate_candidate_intervals_at_j(label_at_i, next_node_id, data)\n",
        "        if not candidate_intervals:\n",
        "            return []\n",
        "        return _filter_dominant_intervals_pareto(candidate_intervals)"
      ],
      "metadata": {
        "id": "vKYeYRI8MPoW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "# Assuming Label class, initialize_depot_label, extend_label_to_new_node,\n",
        "# _calculate_tau_ij_from_paper are defined as in previous steps.\n",
        "\n",
        "def _calculate_phi_L1_L2(label1: Label, label2: Label, data: dict) -> float:\n",
        "    \"\"\"\n",
        "    Calculates phi(L_f^1, L_f^2) as per Algorithm 3.1 in the paper[cite: 345].\n",
        "    This version adapts for a travel distance objective. The term\n",
        "    max{x - L_L_f1(y), 0} in the paper's phi calculation (line 6 of Alg 3.1)\n",
        "    relates to additional waiting time if L1 finishes service earlier at L_L_f1(y)\n",
        "    but has to wait until x. For a pure travel distance objective, this waiting\n",
        "    does not add to the distance cost. So, that term effectively becomes 0.\n",
        "\n",
        "    Thus, phi becomes max(D_L1(y_for_x) - D_L2(z_for_x)), where D is accumulated distance.\n",
        "    Algorithm 3.1 structure is to iterate over intervals of L2 (z) and L1 (y).\n",
        "    \"\"\"\n",
        "    phi_val = -float('inf')\n",
        "    epsilon = 1e-6 # A small value as used in paper's context for strict inequalities\n",
        "\n",
        "    # dominant_forward_start_intervals are (E, L, D)\n",
        "    intervals_L1 = label1.dominant_forward_start_intervals\n",
        "    intervals_L2 = label2.dominant_forward_start_intervals\n",
        "\n",
        "    if not intervals_L1 or not intervals_L2: # Should not happen if labels are valid\n",
        "        return -float('inf')\n",
        "\n",
        "    idx_y = 0\n",
        "    for E_L2_z, L_L2_z, D_L2_z in intervals_L2:\n",
        "        # Find the interval y in L1 that is active for departures x in [E_L2_z, L_L2_z]\n",
        "        # This requires careful handling of how delta_L1(x) is determined.\n",
        "        # Algorithm 3.1 compares an interval z of L2 with inter-interval gaps of L1\n",
        "        # or overlaps with intervals of L1.\n",
        "\n",
        "        # Iterate through L1's intervals/gaps relevant to L2's current interval z\n",
        "        current_L1_idx = 0\n",
        "        temp_max_for_z = -float('inf')\n",
        "\n",
        "        # This loop implements the logic from Algorithm 3.1 lines 3-11 [cite: 345]\n",
        "        # It iterates through y in F_L1 (intervals_L1)\n",
        "        # For each z in F_L2 (current L2 interval)\n",
        "        temp_y_idx = 0\n",
        "        while temp_y_idx < len(intervals_L1):\n",
        "            E_L1_y, L_L1_y, D_L1_y = intervals_L1[temp_y_idx]\n",
        "\n",
        "            # Next interval start for L1, or infinity if y is the last\n",
        "            E_L1_y_plus_1 = intervals_L1[temp_y_idx+1][0] if temp_y_idx + 1 < len(intervals_L1) else float('inf')\n",
        "\n",
        "            # Check overlap: [E_L2_z, L_L2_z] with [E_L1_y, E_L1_y_plus_1[\n",
        "            # This is the \"departure time x\" range considered in Alg 3.1\n",
        "            overlap_start = max(E_L2_z, E_L1_y)\n",
        "            overlap_end = min(L_L2_z, E_L1_y_plus_1 - epsilon)\n",
        "\n",
        "            if overlap_start <= overlap_end: # If there's a relevant segment to check\n",
        "                # x is chosen as the end of this overlapping segment (line 5 of Alg 3.1)\n",
        "                # x = min(L_L2_z, E_L1_y_plus_1 - epsilon)\n",
        "                # For this x, delta_L1(x) = D_L1_y + max(x - L_L1_y, 0)\n",
        "                # For distance, max(x - L_L1_y, 0) is 0. So delta_L1(x) = D_L1_y\n",
        "                # delta_L2(x) = D_L2_z (since x is within L2's interval z by definition of loop)\n",
        "                current_phi_contribution = D_L1_y - D_L2_z\n",
        "                if current_phi_contribution > temp_max_for_z:\n",
        "                    temp_max_for_z = current_phi_contribution\n",
        "\n",
        "            if L_L2_z < E_L1_y_plus_1 - epsilon : # Line 9 of Alg 3.1 condition\n",
        "                break # Go to next start time interval of label L2 (outer loop)\n",
        "\n",
        "            temp_y_idx += 1\n",
        "\n",
        "        if temp_max_for_z > -float('inf'):\n",
        "             if temp_max_for_z > phi_val:\n",
        "                phi_val = temp_max_for_z\n",
        "        # Handle case where L2 interval extends beyond all L1 intervals/gaps\n",
        "        elif temp_y_idx == len(intervals_L1) and E_L2_z <= intervals_L1[-1][1]: # L2_z overlaps with last L1 interval\n",
        "            E_L1_y_last, L_L1_y_last, D_L1_y_last = intervals_L1[-1]\n",
        "            # Check overlap with [E_L1_y_last, L_L1_y_last]\n",
        "            overlap_start = max(E_L2_z, E_L1_y_last)\n",
        "            overlap_end = min(L_L2_z, L_L1_y_last)\n",
        "            if overlap_start <= overlap_end:\n",
        "                 current_phi_contribution = D_L1_y_last - D_L2_z\n",
        "                 if current_phi_contribution > phi_val: # update overall phi_val directly\n",
        "                    phi_val = current_phi_contribution\n",
        "\n",
        "\n",
        "    return phi_val if phi_val > -float('inf') else 0.0 # If no valid comparison, phi might be 0 or some default\n",
        "\n",
        "\n",
        "def check_dominance(label1: Label, label2: Label, data: dict) -> bool:\n",
        "    \"\"\"\n",
        "    Checks if label1 dominates label2 based on Proposition 3.1[cite: 339, 88].\n",
        "    Adapted for travel distance objective.\n",
        "\n",
        "    Args:\n",
        "        label1: The potentially dominating label.\n",
        "        label2: The potentially dominated label.\n",
        "        data: Preprocessed data.\n",
        "\n",
        "    Returns:\n",
        "        True if label1 dominates label2, False otherwise.\n",
        "    \"\"\"\n",
        "    # Condition 1: v(L_f^1) = v(L_f^2)\n",
        "    # This is implicitly true as this function will be called for labels ending at the same node.\n",
        "    if label1.current_node_id != label2.current_node_id:\n",
        "        return False # Should not happen if used correctly\n",
        "\n",
        "    # Condition 2: S(L_f^1) subseteq S_bar(L_f^2) [cite: 339]\n",
        "    # Using simplified S(L_f^1) subseteq S(L_f^2) for elementarity.\n",
        "    # If L1 visits a customer not in L2, L1 cannot dominate L2 if L2 could still visit it.\n",
        "    # For L1 to dominate L2, L1 must be \"more general\" or equally general.\n",
        "    # This means L1 should not have visited *more* specific customers than L2.\n",
        "    # So, label1.visited_customers_set must be a subset of or equal to label2.visited_customers_set\n",
        "    if not label1.visited_customers_set.issubset(label2.visited_customers_set):\n",
        "        return False\n",
        "\n",
        "    # Condition 3: q(L_f^1) <= q(L_f^2) [cite: 339]\n",
        "    if label1.current_load > label2.current_load:\n",
        "        return False\n",
        "\n",
        "    # Condition 4: E_L_f1(1) <= E_L_f2(1) [cite: 340]\n",
        "    if label1.get_earliest_start_of_first_interval() > label2.get_earliest_start_of_first_interval():\n",
        "        return False\n",
        "\n",
        "    # Condition 5: phi(L_f^1, L_f^2) <= pi(L_f^1) - pi(L_f^2) [cite: 340]\n",
        "    # where pi(L_f) is the sum of duals for label L_f.\n",
        "    # The RHS is label1.accumulated_dual_value_sum - label2.accumulated_dual_value_sum\n",
        "    # The paper's proof appendix C.2 uses c(L_f^1) - c(L_f^2) where c(L_f) is sum of duals[cite: 88].\n",
        "\n",
        "    # If all previous conditions hold and L1 is strictly better in one resource\n",
        "    # or has strictly lower dual sum for same resources, phi might not be needed.\n",
        "    # But the paper's BCP includes it.\n",
        "\n",
        "    # For distance, the accumulated_dual_value_sum is the \"sum of duals\".\n",
        "    # The \"cost\" of a path in the objective is distance.\n",
        "    # The condition 5 is about reduced cost comparison.\n",
        "    # Reduced cost of L1 + extension <= Reduced cost of L2 + extension\n",
        "    # (Dist1 + phi) - DualSum1 <= Dist2 - DualSum2 (if phi represents cost difference)\n",
        "    # Dist1 - DualSum1 <= Dist2 - (DualSum2 + phi)\n",
        "    # The paper's condition (Prop 3.1, page 47): phi(L1,L2) <= pi(L1) - pi(L2)\n",
        "    # where pi(L) is sum of dual variables.\n",
        "\n",
        "    # Calculate phi(label1, label2)\n",
        "    # This phi should represent max(cost_L1_extension - cost_L2_extension) for any common extension.\n",
        "    # If cost is just distance, and delta_Lf(x) = D_Lf(y), then phi simplifies.\n",
        "    # Let's use the _calculate_phi_L1_L2 implementation.\n",
        "    phi_1_2 = _calculate_phi_L1_L2(label1, label2, data)\n",
        "\n",
        "    if phi_1_2 > (label1.accumulated_dual_value_sum - label2.accumulated_dual_value_sum) + 1e-6: # Add tolerance for float comparison\n",
        "        return False\n",
        "\n",
        "    # At least one of the conditions (load, earliest_start_time, or effective cost from Cond 5)\n",
        "    # must be strictly better if the visited sets are identical. Or if L1 is a strict subset for visited.\n",
        "    if label1.visited_customers_set == label2.visited_customers_set:\n",
        "        if not (label1.current_load < label2.current_load - 1e-6 or\n",
        "                label1.get_earliest_start_of_first_interval() < label2.get_earliest_start_of_first_interval() - 1e-6 or\n",
        "                (phi_1_2 < (label1.accumulated_dual_value_sum - label2.accumulated_dual_value_sum) - 1e-6) ):\n",
        "            # If all are equal or only phi makes them equal, it might not be a strict dominance\n",
        "            # However, the paper does not require strict inequality for dominance.\n",
        "            # Let's assume non-strict as per paper for now.\n",
        "            pass\n",
        "\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def solve_espprc_forward_labeling(\n",
        "    data: dict,\n",
        "    dual_prices: dict,\n",
        "    alpha_penalty: float,\n",
        "    depot_return_time_limit: float\n",
        "):\n",
        "    \"\"\"\n",
        "    Solves the Elementary Shortest Path Problem with Resource Constraints (ESPPRC)\n",
        "    using a monodirectional forward labeling algorithm.\n",
        "\n",
        "    Args:\n",
        "        data (dict): Preprocessed problem data.\n",
        "        dual_prices (dict): {customer_id: dual_value} from RMP.\n",
        "        alpha_penalty (float): Cost for using a vehicle.\n",
        "        depot_return_time_limit (float): Latest allowed arrival time back at the depot.\n",
        "\n",
        "\n",
        "    Returns:\n",
        "        list: A list of new routes (dicts) with negative reduced costs.\n",
        "              Each route dict: {'path': list_of_nodes, 'cost': travel_distance,\n",
        "                                'customers_served': list_of_customers,\n",
        "                                'reduced_cost': float, 'id': str}\n",
        "    \"\"\"\n",
        "    depot_id = data['depot_info']['id']\n",
        "    customers_list = data['customers_list']\n",
        "    vehicle_capacity = data['vehicle_capacity']\n",
        "\n",
        "    initial_label = initialize_depot_label(data)\n",
        "\n",
        "    # Stores non-dominated labels found so far for each node\n",
        "    # key: node_id, value: list of Label objects\n",
        "    non_dominated_labels_at_node = {node_id: [] for node_id in [depot_id] + customers_list}\n",
        "    non_dominated_labels_at_node[depot_id].append(initial_label)\n",
        "\n",
        "    # Labels to be processed\n",
        "    unprocessed_labels = [initial_label]\n",
        "\n",
        "    generated_negative_routes = []\n",
        "    min_reduced_cost_overall = 0.0 # Track the most negative reduced cost\n",
        "\n",
        "    iteration_count = 0 # Safety break for very long runs\n",
        "\n",
        "    while unprocessed_labels:\n",
        "        iteration_count += 1\n",
        "        # if iteration_count % 100 == 0:\n",
        "        #     print(f\"  ESPPRC Iteration: {iteration_count}, Unprocessed: {len(unprocessed_labels)}, Neg Routes: {len(generated_negative_routes)}\")\n",
        "        # if iteration_count > 20000: # Safety break\n",
        "        #     print(\"  ESPPRC safety break: Max iterations reached.\")\n",
        "        #     break\n",
        "\n",
        "        current_L = unprocessed_labels.pop(0) # FIFO processing for now\n",
        "\n",
        "        # Try to extend to other customers\n",
        "        for next_cust_id in customers_list:\n",
        "            if next_cust_id not in current_L.visited_customers_set:\n",
        "                # 1. Check capacity\n",
        "                if current_L.current_load + data['demands'].get(next_cust_id, 0) <= vehicle_capacity:\n",
        "                    # 2. Generate dominant forward start intervals at next_cust_id\n",
        "                    # extend_label_to_new_node was the previous name\n",
        "                    new_intervals_at_next_node = extend_label_to_new_node(current_L, next_cust_id, data)\n",
        "\n",
        "                    if new_intervals_at_next_node: # Feasible extension found\n",
        "                        new_path_seq = current_L.path_sequence + [next_cust_id]\n",
        "                        new_visited_set = current_L.visited_customers_set.union({next_cust_id})\n",
        "                        new_load = current_L.current_load + data['demands'].get(next_cust_id, 0)\n",
        "                        new_dual_sum = current_L.accumulated_dual_value_sum + dual_prices.get(next_cust_id, 0.0)\n",
        "\n",
        "                        new_label = Label(\n",
        "                            current_node_id=next_cust_id,\n",
        "                            visited_customers_set=new_visited_set,\n",
        "                            current_load=new_load,\n",
        "                            dominant_forward_start_intervals=new_intervals_at_next_node,\n",
        "                            accumulated_dual_value_sum=new_dual_sum,\n",
        "                            path_sequence=new_path_seq\n",
        "                        )\n",
        "\n",
        "                        # Dominance Check (Proposition 3.1)\n",
        "                        is_dominated_by_existing = False\n",
        "                        kept_labels_for_next_node = []\n",
        "                        for existing_label in non_dominated_labels_at_node[next_cust_id]:\n",
        "                            if check_dominance(existing_label, new_label, data):\n",
        "                                is_dominated_by_existing = True\n",
        "                                break\n",
        "                            if not check_dominance(new_label, existing_label, data):\n",
        "                                kept_labels_for_next_node.append(existing_label)\n",
        "\n",
        "                        if not is_dominated_by_existing:\n",
        "                            non_dominated_labels_at_node[next_cust_id] = kept_labels_for_next_node + [new_label]\n",
        "                            unprocessed_labels.append(new_label)\n",
        "                            # Sort unprocessed_labels? e.g. by path length or dual sum for heuristic processing order\n",
        "                            # unprocessed_labels.sort(key=lambda l: len(l.path_sequence))\n",
        "\n",
        "\n",
        "        # Try to extend current_L (ending at current_L.current_node_id) back to the depot\n",
        "        if current_L.current_node_id != depot_id : # Path has at least one customer\n",
        "            path_ending_node_id = current_L.current_node_id\n",
        "\n",
        "            # tau_ij for returning to depot: service_at_path_ending_node + travel_time_to_depot\n",
        "            tau_to_depot = _calculate_tau_ij_from_paper(path_ending_node_id, depot_id, data)\n",
        "            dist_to_depot = data['travel_distances'].get((path_ending_node_id, depot_id), float('inf'))\n",
        "\n",
        "            min_route_distance_for_this_path = float('inf')\n",
        "            feasible_return = False\n",
        "\n",
        "            for E_L_y, L_L_y, D_L_y in current_L.dominant_forward_start_intervals:\n",
        "                # Earliest completion time at path_ending_node_id if service started at E_L_y\n",
        "                # Service starts at E_L_y, duration is service_time. Travel starts after service.\n",
        "                # Arrival at depot: E_L_y (start service) + tau_to_depot\n",
        "                arrival_at_depot = E_L_y + tau_to_depot\n",
        "\n",
        "                if arrival_at_depot <= depot_return_time_limit:\n",
        "                    current_total_dist = D_L_y + dist_to_depot\n",
        "                    if current_total_dist < min_route_distance_for_this_path:\n",
        "                        min_route_distance_for_this_path = current_total_dist\n",
        "                    feasible_return = True\n",
        "\n",
        "            if feasible_return:\n",
        "                # Route cost = total_travel_distance + alpha_penalty\n",
        "                route_cost_C_r = min_route_distance_for_this_path + alpha_penalty\n",
        "                # Reduced cost = C_r - sum(dual_prices for customers in path)\n",
        "                # current_L.accumulated_dual_value_sum already has sum for customers in path_sequence[1:]\n",
        "                sum_duals_for_path = current_L.accumulated_dual_value_sum\n",
        "\n",
        "                reduced_cost = route_cost_C_r - sum_duals_for_path\n",
        "\n",
        "                if reduced_cost < 1e-6 : # Threshold for negativity\n",
        "                    # print(f\"  Found path {current_L.path_sequence + [depot_id]} to depot. Dist: {min_route_distance_for_this_path:.2f}, RC: {reduced_cost:.2f}\")\n",
        "                    route_details = {\n",
        "                        'path': current_L.path_sequence + [depot_id],\n",
        "                        'cost': min_route_distance_for_this_path, # This is pure travel distance\n",
        "                        'customers_served': list(current_L.visited_customers_set),\n",
        "                        'reduced_cost': reduced_cost,\n",
        "                        'id': f\"gen_route_{len(generated_negative_routes)}_{iteration_count}\"\n",
        "                    }\n",
        "                    generated_negative_routes.append(route_details)\n",
        "                if reduced_cost < min_reduced_cost_overall:\n",
        "                    min_reduced_cost_overall = reduced_cost\n",
        "\n",
        "    # Sort by reduced cost to return the best ones first (most negative)\n",
        "    generated_negative_routes.sort(key=lambda r: r['reduced_cost'])\n",
        "    return generated_negative_routes"
      ],
      "metadata": {
        "id": "0HwJBgKFMRIG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional, List, Dict, Tuple, Set\n",
        "\n",
        "def extend_backward_label_to_node(label_at_j: Label, prev_node_id: int, data: dict) -> Optional[Label]:\n",
        "    \"\"\"\n",
        "    Extends a backward label from node j to previous node i.\n",
        "\n",
        "    This is the backward equivalent of forward label extension.\n",
        "    The key difference is that we're going backwards: from j to i.\n",
        "\n",
        "    Returns:\n",
        "        New backward label at node i, or None if extension is infeasible.\n",
        "    \"\"\"\n",
        "    if not label_at_j.is_forward:  # Must be backward label\n",
        "        node_j_id = label_at_j.current_node_id\n",
        "\n",
        "        # Check if prev_node_id is reachable\n",
        "        if prev_node_id in label_at_j.visited_customers_set:\n",
        "            return None\n",
        "\n",
        "        # Check capacity\n",
        "        if prev_node_id != data['depot_id']:\n",
        "            new_load = label_at_j.current_load + data['demands'].get(prev_node_id, 0)\n",
        "            if new_load > data['vehicle_capacity']:\n",
        "                return None\n",
        "        else:\n",
        "            new_load = label_at_j.current_load\n",
        "\n",
        "        # Generate backward intervals at prev_node_id\n",
        "        # Similar to forward but in reverse direction\n",
        "        new_intervals = _generate_backward_intervals_at_i(label_at_j, prev_node_id, data)\n",
        "\n",
        "        if not new_intervals:\n",
        "            return None\n",
        "\n",
        "        # Create new backward label\n",
        "        new_visited = label_at_j.visited_customers_set.copy()\n",
        "        if prev_node_id != data['depot_id']:\n",
        "            new_visited.add(prev_node_id)\n",
        "\n",
        "        new_path = [prev_node_id] + label_at_j.path_sequence\n",
        "\n",
        "        new_dual_sum = label_at_j.accumulated_dual_value_sum\n",
        "        if prev_node_id != data['depot_id']:\n",
        "            new_dual_sum += data.get('dual_prices', {}).get(prev_node_id, 0)\n",
        "\n",
        "        return Label(\n",
        "            current_node_id=prev_node_id,\n",
        "            visited_customers_set=new_visited,\n",
        "            current_load=new_load,\n",
        "            dominant_intervals=new_intervals,\n",
        "            accumulated_dual_value_sum=new_dual_sum,\n",
        "            path_sequence=new_path,\n",
        "            is_forward=False\n",
        "        )\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def _generate_backward_intervals_at_i(label_at_j: Label, node_i_id: int, data: dict) -> list:\n",
        "    \"\"\"\n",
        "    Generates backward start intervals at node i when extending backward from j.\n",
        "\n",
        "    This is the backward equivalent of forward interval generation.\n",
        "    Key: We need to ensure that if we start service at node i within these intervals,\n",
        "    we can reach node j in time.\n",
        "    \"\"\"\n",
        "    node_j_id = label_at_j.current_node_id\n",
        "    generated_intervals = []\n",
        "\n",
        "    # tau_ij includes service time at i and travel time from i to j\n",
        "    service_time_at_i = data['service_times'].get(node_i_id, 0) if node_i_id != data['depot_id'] else 0\n",
        "    travel_time_ij = data['travel_times'].get((node_i_id, node_j_id), float('inf'))\n",
        "    tau_ij = service_time_at_i + travel_time_ij\n",
        "    travel_dist_ij = data['travel_distances'].get((node_i_id, node_j_id), float('inf'))\n",
        "\n",
        "    # Get time windows at node i\n",
        "    time_windows_at_i = data['time_windows_data'].get(node_i_id, [])\n",
        "\n",
        "    # For each backward interval at j, determine feasible intervals at i\n",
        "    for E_Lb_y, L_Lb_y, D_Lb_y in label_at_j.dominant_intervals:\n",
        "        for e_i_t, l_i_t in time_windows_at_i:\n",
        "            # To start service at j at time E_Lb_y, we must finish at i by E_Lb_y - travel_time_ij\n",
        "            # So we must start at i by E_Lb_y - tau_ij\n",
        "            latest_start_at_i = E_Lb_y - tau_ij\n",
        "\n",
        "            # We can start at i as early as e_i_t, but no later than min(l_i_t, latest_start_at_i)\n",
        "            new_E_i = e_i_t\n",
        "            new_L_i = min(l_i_t, latest_start_at_i)\n",
        "\n",
        "            if new_E_i <= new_L_i:\n",
        "                # This is a feasible backward interval\n",
        "                new_D_i = D_Lb_y + travel_dist_ij\n",
        "                generated_intervals.append((new_E_i, new_L_i, new_D_i))\n",
        "\n",
        "    # Filter to keep only dominant intervals\n",
        "    return _filter_dominant_intervals_pareto(generated_intervals)\n",
        "\n",
        "\n",
        "def check_backward_dominance(label1: Label, label2: Label, data: dict) -> bool:\n",
        "    \"\"\"\n",
        "    Checks if backward label1 dominates backward label2 (Proposition 3.2).\n",
        "\n",
        "    Conditions:\n",
        "    1. v(L1_b) = v(L2_b)\n",
        "    2. S(L1_b) ⊆ S_bar(L2_b)\n",
        "    3. q(L1_b) ≤ q(L2_b)\n",
        "    4. L_L1_b(|B_L1_b|) ≥ L_L2_b(|B_L2_b|)\n",
        "    5. φ(L1_b, L2_b) ≤ π(L1_b) - π(L2_b)\n",
        "    \"\"\"\n",
        "    if label1.current_node_id != label2.current_node_id:\n",
        "        return False\n",
        "\n",
        "    # Check if L1 visits only customers that L2 could still visit\n",
        "    if not label1.visited_customers_set.issubset(label2.visited_customers_set):\n",
        "        return False\n",
        "\n",
        "    # Check load\n",
        "    if label1.current_load > label2.current_load:\n",
        "        return False\n",
        "\n",
        "    # Check latest start time of last interval\n",
        "    if label1.get_latest_start_of_last_interval() < label2.get_latest_start_of_last_interval():\n",
        "        return False\n",
        "\n",
        "    # Calculate φ(L1_b, L2_b) - similar to forward but for backward\n",
        "    phi_val = _calculate_phi_backward(label1, label2, data)\n",
        "\n",
        "    if phi_val > (label1.accumulated_dual_value_sum - label2.accumulated_dual_value_sum) + 1e-6:\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def _calculate_phi_backward(label1: Label, label2: Label, data: dict) -> float:\n",
        "    \"\"\"\n",
        "    Calculates φ(L1_b, L2_b) for backward labels.\n",
        "    Similar to forward phi calculation but adapted for backward direction.\n",
        "    \"\"\"\n",
        "    phi_val = -float('inf')\n",
        "    epsilon = 1e-6\n",
        "\n",
        "    intervals_L1 = label1.dominant_intervals\n",
        "    intervals_L2 = label2.dominant_intervals\n",
        "\n",
        "    if not intervals_L1 or not intervals_L2:\n",
        "        return -float('inf')\n",
        "\n",
        "    # Similar logic to forward but adapted for backward\n",
        "    for E_L2_z, L_L2_z, D_L2_z in intervals_L2:\n",
        "        for idx, (E_L1_y, L_L1_y, D_L1_y) in enumerate(intervals_L1):\n",
        "            # Check overlap and calculate phi contribution\n",
        "            # This is simplified - full implementation would follow Algorithm 3.1 adapted for backward\n",
        "            overlap_start = max(E_L2_z, E_L1_y)\n",
        "            overlap_end = min(L_L2_z, L_L1_y)\n",
        "\n",
        "            if overlap_start <= overlap_end:\n",
        "                # For backward, the phi calculation is similar but considers arrival times\n",
        "                current_phi = D_L2_z - D_L1_y\n",
        "                if current_phi > phi_val:\n",
        "                    phi_val = current_phi\n",
        "\n",
        "    return phi_val if phi_val > -float('inf') else 0.0\n",
        "\n",
        "\n",
        "def merge_forward_backward_labels(forward_label: Label, backward_label: Label, data: dict) -> Optional[dict]:\n",
        "    \"\"\"\n",
        "    Merges a forward and backward label to create a complete route (Algorithm C.2).\n",
        "\n",
        "    Returns:\n",
        "        Dict with route information if merge is feasible, None otherwise.\n",
        "    \"\"\"\n",
        "    # Check merge conditions\n",
        "    if forward_label.current_node_id == backward_label.current_node_id:\n",
        "        return None  # Can't merge at same node\n",
        "\n",
        "    # Check if arc exists\n",
        "    if (forward_label.current_node_id, backward_label.current_node_id) not in data['travel_times']:\n",
        "        return None\n",
        "\n",
        "    # Check no shared customers\n",
        "    if forward_label.visited_customers_set.intersection(backward_label.visited_customers_set):\n",
        "        return None\n",
        "\n",
        "    # Check capacity\n",
        "    if forward_label.current_load + backward_label.current_load > data['vehicle_capacity']:\n",
        "        return None\n",
        "\n",
        "    # Check time feasibility\n",
        "    tau_ij = _calculate_tau_ij_from_paper(forward_label.current_node_id,\n",
        "                                          backward_label.current_node_id, data)\n",
        "\n",
        "    # Find feasible interval combinations\n",
        "    min_route_distance = float('inf')\n",
        "    feasible_merge = False\n",
        "\n",
        "    for E_Lf_y, L_Lf_y, D_Lf_y in forward_label.dominant_intervals:\n",
        "        for E_Lb_z, L_Lb_z, D_Lb_z in backward_label.dominant_intervals:\n",
        "            # Check if we can depart from forward node and arrive at backward node in time\n",
        "            earliest_arrival_at_backward = E_Lf_y + tau_ij\n",
        "            latest_arrival_at_backward = L_Lf_y + tau_ij\n",
        "\n",
        "            # We need to arrive before L_Lb_z to start service by then\n",
        "            if earliest_arrival_at_backward <= L_Lb_z:\n",
        "                # Calculate waiting time if any\n",
        "                actual_start_at_backward = max(earliest_arrival_at_backward, E_Lb_z)\n",
        "                waiting_time = max(0, E_Lb_z - latest_arrival_at_backward)\n",
        "\n",
        "                # Total distance for this combination\n",
        "                dist_ij = data['travel_distances'][(forward_label.current_node_id,\n",
        "                                                   backward_label.current_node_id)]\n",
        "                total_dist = D_Lf_y + dist_ij + D_Lb_z\n",
        "\n",
        "                if total_dist < min_route_distance:\n",
        "                    min_route_distance = total_dist\n",
        "                    feasible_merge = True\n",
        "\n",
        "    if not feasible_merge:\n",
        "        return None\n",
        "\n",
        "    # Create complete path\n",
        "    complete_path = forward_label.path_sequence + backward_label.path_sequence\n",
        "\n",
        "    # Calculate route details\n",
        "    route_details = {\n",
        "        'path': complete_path,\n",
        "        'cost': min_route_distance,  # Pure travel distance\n",
        "        'customers_served': list(forward_label.visited_customers_set.union(\n",
        "                                backward_label.visited_customers_set)),\n",
        "        'reduced_cost': None,  # Will be calculated later\n",
        "        'id': f\"route_{len(complete_path)}_{forward_label.current_node_id}_{backward_label.current_node_id}\"\n",
        "    }\n",
        "\n",
        "    return route_details\n",
        "\n",
        "\n",
        "def solve_espprc_bidirectional(data: dict,\n",
        "                               dual_prices: dict,\n",
        "                               alpha_penalty: float,\n",
        "                               max_labels: int = 50000) -> List[dict]:\n",
        "    \"\"\"\n",
        "    Solves ESPPRC using bidirectional labeling algorithm.\n",
        "\n",
        "    Returns:\n",
        "        List of routes with negative reduced cost.\n",
        "    \"\"\"\n",
        "    depot_id = data['depot_id']\n",
        "    customers_list = data['customers_list']\n",
        "\n",
        "    # Initialize forward and backward labels at depot\n",
        "    initial_forward = initialize_depot_label(data)\n",
        "    initial_backward = initialize_depot_label(data)\n",
        "    initial_backward.is_forward = False\n",
        "\n",
        "    # Storage for non-dominated labels\n",
        "    forward_labels = {node_id: [] for node_id in [depot_id] + customers_list}\n",
        "    backward_labels = {node_id: [] for node_id in [depot_id] + customers_list}\n",
        "\n",
        "    forward_labels[depot_id].append(initial_forward)\n",
        "    backward_labels[depot_id].append(initial_backward)\n",
        "\n",
        "    # Unprocessed labels\n",
        "    forward_unprocessed = [initial_forward]\n",
        "    backward_unprocessed = [initial_backward]\n",
        "\n",
        "    # Track bounds for merging decision\n",
        "    min_forward_E = 0  # Minimum E_Lf(1) among forward labels\n",
        "    max_backward_L = data['depot_info']['time_window'][1]  # Maximum L_Lb(|B_Lb|)\n",
        "\n",
        "    generated_routes = []\n",
        "    total_labels_processed = 0\n",
        "\n",
        "    while (forward_unprocessed or backward_unprocessed) and total_labels_processed < max_labels:\n",
        "        # Process forward labels\n",
        "        if forward_unprocessed and total_labels_processed < max_labels // 2:\n",
        "            current_forward = forward_unprocessed.pop(0)\n",
        "            total_labels_processed += 1\n",
        "\n",
        "            # Extend to customers\n",
        "            for next_cust_id in customers_list:\n",
        "                if next_cust_id not in current_forward.visited_customers_set:\n",
        "                    # Check basic feasibility\n",
        "                    if (current_forward.current_load + data['demands'].get(next_cust_id, 0) <=\n",
        "                        data['vehicle_capacity']):\n",
        "                        # Generate new forward intervals\n",
        "                        new_intervals = extend_label_to_new_node(current_forward, next_cust_id, data)\n",
        "\n",
        "                        if new_intervals:\n",
        "                            # Create new forward label\n",
        "                            new_path = current_forward.path_sequence + [next_cust_id]\n",
        "                            new_visited = current_forward.visited_customers_set.union({next_cust_id})\n",
        "                            new_load = current_forward.current_load + data['demands'][next_cust_id]\n",
        "                            new_dual_sum = current_forward.accumulated_dual_value_sum + dual_prices.get(next_cust_id, 0)\n",
        "\n",
        "                            new_forward_label = Label(\n",
        "                                current_node_id=next_cust_id,\n",
        "                                visited_customers_set=new_visited,\n",
        "                                current_load=new_load,\n",
        "                                dominant_intervals=new_intervals,\n",
        "                                accumulated_dual_value_sum=new_dual_sum,\n",
        "                                path_sequence=new_path,\n",
        "                                is_forward=True\n",
        "                            )\n",
        "\n",
        "                            # Check dominance\n",
        "                            is_dominated = False\n",
        "                            labels_to_remove = []\n",
        "\n",
        "                            for existing_label in forward_labels[next_cust_id]:\n",
        "                                if check_dominance(existing_label, new_forward_label, data):\n",
        "                                    is_dominated = True\n",
        "                                    break\n",
        "                                elif check_dominance(new_forward_label, existing_label, data):\n",
        "                                    labels_to_remove.append(existing_label)\n",
        "\n",
        "                            if not is_dominated:\n",
        "                                # Remove dominated labels\n",
        "                                for label in labels_to_remove:\n",
        "                                    forward_labels[next_cust_id].remove(label)\n",
        "                                    if label in forward_unprocessed:\n",
        "                                        forward_unprocessed.remove(label)\n",
        "\n",
        "                                # Add new label\n",
        "                                forward_labels[next_cust_id].append(new_forward_label)\n",
        "                                forward_unprocessed.append(new_forward_label)\n",
        "\n",
        "                                # Update min forward E\n",
        "                                min_forward_E = min(min_forward_E, new_forward_label.get_earliest_start_of_first_interval())\n",
        "\n",
        "        # Process backward labels\n",
        "        if backward_unprocessed and total_labels_processed < max_labels:\n",
        "            current_backward = backward_unprocessed.pop(0)\n",
        "            total_labels_processed += 1\n",
        "\n",
        "            # Extend to customers (backwards)\n",
        "            for prev_cust_id in customers_list:\n",
        "                if prev_cust_id not in current_backward.visited_customers_set:\n",
        "                    # Use backward extension\n",
        "                    new_backward_label = extend_backward_label_to_node(current_backward, prev_cust_id, data)\n",
        "\n",
        "                    if new_backward_label:\n",
        "                        # Check backward dominance\n",
        "                        is_dominated = False\n",
        "                        labels_to_remove = []\n",
        "\n",
        "                        for existing_label in backward_labels[prev_cust_id]:\n",
        "                            if check_backward_dominance(existing_label, new_backward_label, data):\n",
        "                                is_dominated = True\n",
        "                                break\n",
        "                            elif check_backward_dominance(new_backward_label, existing_label, data):\n",
        "                                labels_to_remove.append(existing_label)\n",
        "\n",
        "                        if not is_dominated:\n",
        "                            # Remove dominated labels\n",
        "                            for label in labels_to_remove:\n",
        "                                backward_labels[prev_cust_id].remove(label)\n",
        "                                if label in backward_unprocessed:\n",
        "                                    backward_unprocessed.remove(label)\n",
        "\n",
        "                            # Add new label\n",
        "                            backward_labels[prev_cust_id].append(new_backward_label)\n",
        "                            backward_unprocessed.append(new_backward_label)\n",
        "\n",
        "                            # Update max backward L\n",
        "                            max_backward_L = max(max_backward_L,\n",
        "                                               new_backward_label.get_latest_start_of_last_interval())\n",
        "\n",
        "        # Check merging condition\n",
        "        if min_forward_E > max_backward_L or total_labels_processed >= max_labels // 2:\n",
        "            # Merge forward and backward labels\n",
        "            for forward_node_id, forward_label_list in forward_labels.items():\n",
        "                if forward_node_id == depot_id:\n",
        "                    continue\n",
        "\n",
        "                for forward_label in forward_label_list:\n",
        "                    # Try to merge with backward labels at different nodes\n",
        "                    for backward_node_id, backward_label_list in backward_labels.items():\n",
        "                        if backward_node_id == depot_id or backward_node_id == forward_node_id:\n",
        "                            continue\n",
        "\n",
        "                        for backward_label in backward_label_list:\n",
        "                            # Try to merge\n",
        "                            route_details = merge_forward_backward_labels(forward_label,\n",
        "                                                                        backward_label, data)\n",
        "\n",
        "                            if route_details:\n",
        "                                # Calculate reduced cost\n",
        "                                route_cost = route_details['cost'] + alpha_penalty\n",
        "                                sum_duals = (forward_label.accumulated_dual_value_sum +\n",
        "                                           backward_label.accumulated_dual_value_sum)\n",
        "                                reduced_cost = route_cost - sum_duals\n",
        "\n",
        "                                route_details['reduced_cost'] = reduced_cost\n",
        "\n",
        "                                if reduced_cost < -1e-6:\n",
        "                                    generated_routes.append(route_details)\n",
        "\n",
        "            # Clear processed labels to make room for new ones\n",
        "            if total_labels_processed >= max_labels:\n",
        "                break\n",
        "\n",
        "    # Sort routes by reduced cost\n",
        "    generated_routes.sort(key=lambda r: r['reduced_cost'])\n",
        "\n",
        "    return generated_routes"
      ],
      "metadata": {
        "id": "ev7G-CNAMSxB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def solve_espprc_with_multiple_time_windows(\n",
        "    customers_list, demands, service_times, time_windows_data,\n",
        "    coordinates_data, vehicle_capacity, speed, alpha_penalty,\n",
        "    dual_prices, depot_id=0, depot_return_time_limit=float('inf')):\n",
        "    \"\"\"\n",
        "    Modified to use forward labeling for testing.\n",
        "    \"\"\"\n",
        "    data = {\n",
        "        'customers_list': customers_list,\n",
        "        'demands': demands,\n",
        "        'service_times': service_times,\n",
        "        'time_windows_data': time_windows_data,\n",
        "        'coordinates_data': coordinates_data,\n",
        "        'vehicle_capacity': vehicle_capacity,\n",
        "        'speed': speed,\n",
        "        'depot_info': {\n",
        "            'id': depot_id,\n",
        "            'time_window': (0, depot_return_time_limit), # Use depot_return_time_limit\n",
        "            'demand': 0,\n",
        "            'service_time': 0\n",
        "        },\n",
        "        'dual_prices': dual_prices,\n",
        "        'travel_times': {},\n",
        "        'travel_distances': {}\n",
        "    }\n",
        "\n",
        "    # Populate travel times and distances\n",
        "    data['travel_distances'] = calculate_distance_matrix(coordinates_data)\n",
        "    data['travel_times'] = calculate_travel_time_matrix(data['travel_distances'], speed)\n",
        "\n",
        "    # Call forward labeling ESPPRC\n",
        "    routes = solve_espprc_forward_labeling(data, dual_prices, alpha_penalty, depot_return_time_limit)\n",
        "\n",
        "    if routes:\n",
        "        best_route = routes[0] # Already sorted by reduced cost\n",
        "        return best_route, best_route['reduced_cost']\n",
        "    else:\n",
        "        return None, float('inf')\n",
        "\n",
        "# --- Utility functions for the test instance ---\n",
        "def calculate_distance_matrix(coordinates: Dict[int, Tuple[float, float]]) -> Dict[Tuple[int, int], float]:\n",
        "    distances = {}\n",
        "    ids = list(coordinates.keys())\n",
        "    for i in range(len(ids)):\n",
        "        for j in range(len(ids)):\n",
        "            id1 = ids[i]\n",
        "            id2 = ids[j]\n",
        "            if id1 == id2:\n",
        "                distances[(id1, id2)] = 0.0\n",
        "            else:\n",
        "                coord1 = coordinates[id1]\n",
        "                coord2 = coordinates[id2]\n",
        "                dist = math.sqrt((coord1[0] - coord2[0])**2 + (coord1[1] - coord2[1])**2)\n",
        "                distances[(id1, id2)] = dist\n",
        "    return distances\n",
        "\n",
        "def calculate_travel_time_matrix(distances: Dict[Tuple[int, int], float], speed: float) -> Dict[Tuple[int, int], float]:\n",
        "    travel_times = {}\n",
        "    for pair, dist in distances.items():\n",
        "        travel_times[pair] = dist / speed\n",
        "    return travel_times"
      ],
      "metadata": {
        "id": "TKv8BeTmMaXf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Instance Data\n",
        "customer_ids = [1, 2]\n",
        "depot_id = 0\n",
        "\n",
        "coordinates = {\n",
        "    depot_id: (0, 0),\n",
        "    1: (10, 0),\n",
        "    2: (0, 20),\n",
        "}\n",
        "\n",
        "demands_kg = {\n",
        "    depot_id: 0,\n",
        "    1: 50,\n",
        "    2: 70,\n",
        "}\n",
        "\n",
        "# Service times in minutes\n",
        "service_times_min = {\n",
        "    depot_id: 0,\n",
        "    1: 10,\n",
        "    2: 15,\n",
        "}\n",
        "\n",
        "# Time windows: list of (start_min, end_min) from midnight\n",
        "time_windows_min = {\n",
        "    depot_id: [(0, 1000)], # Depot operational window\n",
        "    1: [(60, 120), (180, 240)],\n",
        "    2: [(90, 150)],\n",
        "}\n",
        "\n",
        "vehicle_capacity_kg = 150.0\n",
        "truck_speed = 1.0 # distance units per minute\n",
        "alpha_penalty_dist_equivalent = 25.0\n",
        "max_cg_iter = 3\n",
        "depot_latest_return_time = 1000 # Matches depot window end\n",
        "\n",
        "# --- Calculate distances and travel times for the instance ---\n",
        "dist_matrix = calculate_distance_matrix(coordinates)\n",
        "time_matrix = calculate_travel_time_matrix(dist_matrix, truck_speed)\n",
        "\n",
        "# --- Prepare initial routes ---\n",
        "# For simplicity, let's create very basic initial routes.\n",
        "# Route 1: Depot -> Cust 1 -> Depot\n",
        "# Route 2: Depot -> Cust 2 -> Depot\n",
        "# We need their *travel distance* for the 'cost' field.\n",
        "# Feasibility of these initial routes regarding time windows is important.\n",
        "# We can use your evaluate_route function if it's robust enough for simple paths,\n",
        "# or manually find a feasible cost.\n",
        "\n",
        "# Prepare data for evaluate_route\n",
        "eval_data = {\n",
        "    'customers_list': customer_ids,\n",
        "    'demands': demands_kg,\n",
        "    'service_times': service_times_min,\n",
        "    'time_windows_data': time_windows_min,\n",
        "    'coordinates_data': coordinates,\n",
        "    'vehicle_capacity': vehicle_capacity_kg,\n",
        "    'speed': truck_speed,\n",
        "    'depot_id': depot_id,\n",
        "    'depot_info': {\n",
        "        'id': depot_id,\n",
        "        'time_window': time_windows_min[depot_id][0],\n",
        "        'demand': 0,\n",
        "        'service_time': 0\n",
        "    },\n",
        "    'travel_times': time_matrix,\n",
        "    'travel_distances': dist_matrix\n",
        "}\n",
        "\n",
        "# Try to find feasible initial routes (requires careful departure time selection)\n",
        "# For this test, let's assume some feasible costs if evaluate_route is too complex to debug now\n",
        "# Cost for D-1-D: dist(0,1)+dist(1,0) = 10+10=20.\n",
        "#   Depot (0) -> Cust 1 (10,0) -> Depot (0,0).\n",
        "#   Depart depot at 0. Arrive C1 at 0+dist(0,1)=10. C1 TWs: [(60,120), (180,240)]. Must wait.\n",
        "#   If service starts at 60. Finishes at 60+10(service)=70.\n",
        "#   Depart C1 at 70. Arrive depot at 70+dist(1,0)=70+10=80. Feasible.\n",
        "cost1_eval = evaluate_route(route_sequence=[1], departure_time=0, data=eval_data)\n",
        "if cost1_eval is None: # Try a later departure for C1's second window\n",
        "    cost1_eval = evaluate_route(route_sequence=[1], departure_time=180-service_times_min[1]-time_matrix[(0,1)]-1, data=eval_data) # Try to arrive just before 180\n",
        "if cost1_eval is None:\n",
        "    print(\"Warning: Could not find feasible initial route for Cust 1 automatically. Using raw distance.\")\n",
        "    cost1_eval = dist_matrix[(0,1)] + dist_matrix[(1,0)]\n",
        "\n",
        "\n",
        "# Cost for D-2-D: dist(0,2)+dist(2,0) = 20+20=40\n",
        "#   Depot (0) -> Cust 2 (0,20) -> Depot (0,0)\n",
        "#   Depart depot at 0. Arrive C2 at 0+dist(0,2)=20. C2 TW: [(90,150)]. Must wait.\n",
        "#   Service starts at 90. Finishes at 90+15(service)=105.\n",
        "#   Depart C2 at 105. Arrive depot at 105+dist(2,0)=105+20=125. Feasible.\n",
        "cost2_eval = evaluate_route(route_sequence=[2], departure_time=0, data=eval_data)\n",
        "if cost2_eval is None:\n",
        "    print(\"Warning: Could not find feasible initial route for Cust 2 automatically. Using raw distance.\")\n",
        "    cost2_eval = dist_matrix[(0,2)] + dist_matrix[(2,0)]\n",
        "\n",
        "initial_routes_for_solver = [\n",
        "    {'id': 'initial_A', 'cost': cost1_eval if cost1_eval is not None else 20.0, 'customers_served': [1]},\n",
        "    {'id': 'initial_B', 'cost': cost2_eval if cost2_eval is not None else 40.0, 'customers_served': [2]},\n",
        "]\n",
        "print(f\"Initial Route A cost (Cust 1): {initial_routes_for_solver[0]['cost']}\")\n",
        "print(f\"Initial Route B cost (Cust 2): {initial_routes_for_solver[1]['cost']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcjuXQg9N7Nb",
        "outputId": "6e3f5842-0507-47f4-da03-461f7a2ee3b2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Route A cost (Cust 1): 20.0\n",
            "Initial Route B cost (Cust 2): 40.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import gurobipy as gp\n",
        "from gurobipy import GRB\n",
        "from typing import List, Dict, Tuple, Optional, Set # Added Set for type hinting\n",
        "\n",
        "# --- Main test execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting Column Generation Test...\")\n",
        "\n",
        "    # Ensure all data uses the correct IDs (depot_id for depot, customer_ids for customers)\n",
        "    # The 'customers_list' for RMP and ESPPRC should only contain actual customer IDs, not the depot.\n",
        "\n",
        "    final_model, selected_routes, all_generated_routes = column_generation_solver(\n",
        "        initial_routes=initial_routes_for_solver,\n",
        "        customers_list=customer_ids, # Pass only customer IDs\n",
        "        demands=demands_kg,\n",
        "        service_times=service_times_min,\n",
        "        time_windows_data=time_windows_min,\n",
        "        coordinates_data=coordinates,\n",
        "        vehicle_capacity=vehicle_capacity_kg,\n",
        "        speed=truck_speed,\n",
        "        alpha_penalty=alpha_penalty_dist_equivalent,\n",
        "        max_cg_iterations=max_cg_iter\n",
        "    )\n",
        "\n",
        "    if selected_routes:\n",
        "        print(\"\\n--- Test Run Completed Successfully ---\")\n",
        "        print(f\"Number of selected routes: {len(selected_routes)}\")\n",
        "        total_dist = sum(r['cost'] for r in selected_routes)\n",
        "        print(f\"Total travel distance of selected routes: {total_dist:.2f}\")\n",
        "        obj_val_check = total_dist + alpha_penalty_dist_equivalent * len(selected_routes)\n",
        "        print(f\"Calculated Objective (dist + alpha*veh): {obj_val_check:.2f}\")\n",
        "        if final_model:\n",
        "             print(f\"Gurobi Master IP Objective: {final_model.ObjVal:.2f}\")\n",
        "\n",
        "        for i, r_detail in enumerate(selected_routes):\n",
        "             print(f\"  Selected Route {i+1} (ID {r_detail.get('id', 'N/A')}):\")\n",
        "             print(f\"    Customers: {r_detail['customers_served']}\")\n",
        "             print(f\"    Travel Distance (cost): {r_detail['cost']:.2f}\")\n",
        "             # You might want to re-evaluate the final selected routes with evaluate_route\n",
        "             # to verify their true feasibility and timing details.\n",
        "    else:\n",
        "        print(\"\\n--- Test Run Did Not Produce a Final Solution ---\")\n",
        "\n",
        "    print(f\"\\nTotal routes in pool at end: {len(all_generated_routes)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17DjGZJMN-hT",
        "outputId": "17478a16-ac2e-4531-ffa8-e73b774e52e5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Column Generation Test...\n",
            "\n",
            "--- Column Generation Iteration: 1 ---\n",
            "Current number of routes in RMP: 2\n",
            "Restricted license - for non-production use only - expires 2026-11-23\n",
            "RMP Objective Value: 110.00\n",
            "Found new route with reduced cost: -32.64\n",
            "\n",
            "--- Column Generation Iteration: 2 ---\n",
            "Current number of routes in RMP: 3\n",
            "RMP Objective Value: 77.36\n",
            "No new route with negative reduced cost found. LP optimal for current columns.\n",
            "\n",
            "--- Column Generation Finished ---\n",
            "Final number of routes generated: 3\n",
            "\n",
            "Solving final Master Problem as IP...\n",
            "Set parameter WLSAccessID\n",
            "Set parameter WLSSecret\n",
            "Set parameter LicenseID to value 2652745\n",
            "Set parameter Threads to value 95\n",
            "Academic license 2652745 - for non-commercial use only - registered to di___@student.sdu.dk\n",
            "Master IP Objective: 77.36067977499789\n",
            "Number of vehicles/routes used: 1\n",
            "\n",
            "Final Solution from Master IP:\n",
            "  Objective (IP): 77.36 (This includes alpha in C_r)\n",
            "  Calculated objective (dist + alpha*veh): 77.36\n",
            "  Number of vehicles: 1\n",
            "  Total travel distance: 52.36\n",
            "\n",
            "--- Test Run Completed Successfully ---\n",
            "Number of selected routes: 1\n",
            "Total travel distance of selected routes: 52.36\n",
            "Calculated Objective (dist + alpha*veh): 77.36\n",
            "Gurobi Master IP Objective: 77.36\n",
            "  Selected Route 1 (ID gen_route_2_4):\n",
            "    Customers: [1, 2]\n",
            "    Travel Distance (cost): 52.36\n",
            "\n",
            "Total routes in pool at end: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Make sure all your Python functions (Label class, solvers, data loaders, etc.)\n",
        "# have been defined and executed in previous cells in this Colab notebook.\n",
        "\n",
        "# Step 2: Upload your instance files\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "print(\"Please upload your instance files (e.g., instance_coordinates.txt, instance_demand.txt, etc.)\")\n",
        "print(\"Ensure they match the names expected by your 'load_and_preprocess_data' function (e.g., 'instance_*.txt')\")\n",
        "\n",
        "# Clean up any old files if they exist to avoid confusion\n",
        "file_prefixes_to_remove = [\"instance_coordinates.txt\", \"instance_demand.txt\", \"instance_service_time.txt\", \"instance_time_windows.txt\"]\n",
        "for f_name in file_prefixes_to_remove:\n",
        "    if os.path.exists(f\"/content/{f_name}\"):\n",
        "        os.remove(f\"/content/{f_name}\")\n",
        "        print(f\"Removed old /content/{f_name}\")\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print(f'User uploaded file \"{fn}\" with length {len(uploaded[fn])} bytes')\n",
        "\n",
        "# Verify that the expected files were uploaded (adjust instance_name if different)\n",
        "INSTANCE_NAME_PREFIX = \"instance\" # This should match the prefix of your uploaded files\n",
        "required_files = [\n",
        "    f\"{INSTANCE_NAME_PREFIX}_coordinates.txt\",\n",
        "    f\"{INSTANCE_NAME_PREFIX}_demand.txt\",\n",
        "    f\"{INSTANCE_NAME_PREFIX}_service_time.txt\",\n",
        "    f\"{INSTANCE_NAME_PREFIX}_time_windows.txt\"\n",
        "]\n",
        "all_files_present = True\n",
        "for req_file in required_files:\n",
        "    if not os.path.exists(f\"/content/{req_file}\"):\n",
        "        print(f\"ERROR: Required file /content/{req_file} was not uploaded or has a different name.\")\n",
        "        all_files_present = False\n",
        "\n",
        "if not all_files_present:\n",
        "    print(\"Please re-run the cell and ensure all required files with correct names are uploaded.\")\n",
        "else:\n",
        "    print(\"\\nAll required files seem to be uploaded. Proceeding with the test run...\")\n",
        "\n",
        "    # Step 3: Define parameters and run the test\n",
        "    # (Assuming all necessary functions like load_and_preprocess_data, evaluate_route,\n",
        "    # column_generation_solver, etc., are already defined in your notebook)\n",
        "\n",
        "    if __name__ == \"__main__\": # Standard practice, though in Colab cells run sequentially\n",
        "        print(\"\\nStarting Column Generation Test (4 Customers with Uploaded Files)...\")\n",
        "\n",
        "        # Parameters for this test run\n",
        "        test_depot_id = 0\n",
        "        test_vehicle_capacity = 250.0\n",
        "        test_speed_dist_per_hr = 60.0 # Results in 1.0 dist_unit/min for travel_time_matrix if dist/hr is expected\n",
        "        test_alpha_penalty = 50.0\n",
        "        test_max_cg_iter = 10\n",
        "        test_depot_latest_return = 24 * 60 # minutes\n",
        "\n",
        "        # Load data using your functions\n",
        "        # The base_path will be \"/content/\" where Colab uploads files by default\n",
        "        data_4_cust = load_and_preprocess_data(\n",
        "            base_path=\"/content\",\n",
        "            instance_name=INSTANCE_NAME_PREFIX, # Use the prefix you defined\n",
        "            speed=test_speed_dist_per_hr,\n",
        "            vehicle_capacity=test_vehicle_capacity,\n",
        "            depot_id=test_depot_id\n",
        "        )\n",
        "\n",
        "        # Prepare initial routes (single customer routes)\n",
        "        initial_routes_for_solver = []\n",
        "        if data_4_cust and data_4_cust.get('customers_list'): # Check if data loaded correctly\n",
        "            for cust_id in data_4_cust['customers_list']:\n",
        "                feasible_cost = None\n",
        "                # Try departing early\n",
        "                cost = evaluate_route(route_sequence=[cust_id], departure_time=0, data=data_4_cust)\n",
        "                if cost is not None:\n",
        "                    feasible_cost = cost\n",
        "                else: # Try departing just in time for the first window\n",
        "                    cust_tws = data_4_cust['time_windows_data'].get(cust_id, [])\n",
        "                    if cust_tws:\n",
        "                        first_tw_start = cust_tws[0][0]\n",
        "                        travel_to_cust = data_4_cust['travel_times'].get((test_depot_id, cust_id), 0)\n",
        "                        depart_depot_target = first_tw_start - travel_to_cust\n",
        "                        cost = evaluate_route(route_sequence=[cust_id], departure_time=max(0, depart_depot_target), data=data_4_cust)\n",
        "                        if cost is not None:\n",
        "                            feasible_cost = cost\n",
        "\n",
        "                if feasible_cost is not None:\n",
        "                    initial_routes_for_solver.append({\n",
        "                        'id': f'initial_cust_{cust_id}',\n",
        "                        'cost': feasible_cost,\n",
        "                        'customers_served': [cust_id]\n",
        "                    })\n",
        "                    print(f\"Initial route for customer {cust_id} cost: {feasible_cost:.2f}\")\n",
        "                else:\n",
        "                    dist_to = data_4_cust['travel_distances'].get((test_depot_id, cust_id), 0)\n",
        "                    dist_from = data_4_cust['travel_distances'].get((cust_id, test_depot_id), 0)\n",
        "                    raw_cost = dist_to + dist_from\n",
        "                    initial_routes_for_solver.append({\n",
        "                        'id': f'initial_cust_{cust_id}_raw',\n",
        "                        'cost': raw_cost,\n",
        "                        'customers_served': [cust_id]\n",
        "                    })\n",
        "                    print(f\"Warning: Could not find feasible initial route for Cust {cust_id} via evaluate_route. Using raw distance: {raw_cost:.2f}\")\n",
        "\n",
        "            if not initial_routes_for_solver and data_4_cust['customers_list']:\n",
        "                 print(\"Error: No initial routes could be created, but customers exist. Aborting CG.\")\n",
        "            else:\n",
        "                # Run the column generation solver\n",
        "                final_model, selected_routes, all_generated_routes = column_generation_solver(\n",
        "                    initial_routes=initial_routes_for_solver,\n",
        "                    customers_list=data_4_cust['customers_list'],\n",
        "                    demands=data_4_cust['demands'],\n",
        "                    service_times=data_4_cust['service_times'],\n",
        "                    time_windows_data=data_4_cust['time_windows_data'],\n",
        "                    coordinates_data=data_4_cust['coordinates_data'],\n",
        "                    vehicle_capacity=test_vehicle_capacity,\n",
        "                    speed=test_speed_dist_per_hr,\n",
        "                    alpha_penalty=test_alpha_penalty,\n",
        "                    max_cg_iterations=test_max_cg_iter\n",
        "                )\n",
        "\n",
        "                if selected_routes:\n",
        "                    print(\"\\n--- Test Run (4 Customers with Uploaded Files) Completed Successfully ---\")\n",
        "                    print(f\"Number of selected routes: {len(selected_routes)}\")\n",
        "                    total_dist = sum(r['cost'] for r in selected_routes)\n",
        "                    print(f\"Total travel distance of selected routes: {total_dist:.2f}\")\n",
        "\n",
        "                    if final_model and hasattr(final_model, 'ObjVal') and final_model.SolCount > 0:\n",
        "                         obj_val_check = total_dist + test_alpha_penalty * len(selected_routes)\n",
        "                         print(f\"Calculated Objective (dist + alpha*veh): {obj_val_check:.2f}\")\n",
        "                         print(f\"Gurobi Master IP Objective: {final_model.ObjVal:.2f}\")\n",
        "\n",
        "                    for i, r_detail in enumerate(selected_routes):\n",
        "                         print(f\"  Selected Route {i+1} (ID {r_detail.get('id', 'N/A')}):\")\n",
        "                         print(f\"    Customers: {r_detail['customers_served']}\")\n",
        "                         print(f\"    Travel Distance (cost): {r_detail['cost']:.2f}\")\n",
        "                elif final_model: # Model exists but no selected routes\n",
        "                    print(\"\\n--- Test Run (4 Customers with Uploaded Files) Did Not Produce a Feasible Solution via Master IP ---\")\n",
        "                    print(f\"Final IP Model status: {final_model.status}\")\n",
        "                else: # No model and no routes\n",
        "                    print(\"\\n--- Test Run (4 Customers with Uploaded Files) Did Not Produce a Final Solution (CG failed) ---\")\n",
        "\n",
        "\n",
        "                if all_generated_routes:\n",
        "                    print(f\"\\nTotal routes in pool at end: {len(all_generated_routes)}\")\n",
        "                else:\n",
        "                    print(\"\\nNo routes in pool at end (or CG did not complete to that stage).\")\n",
        "        else:\n",
        "            print(\"Error: Data loading failed or no customers found. Cannot proceed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 930
        },
        "id": "Oj2KnXKhVvwW",
        "outputId": "9fcae254-e473-4cfe-a575-a791add8e8fe"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your instance files (e.g., instance_coordinates.txt, instance_demand.txt, etc.)\n",
            "Ensure they match the names expected by your 'load_and_preprocess_data' function (e.g., 'instance_*.txt')\n",
            "Removed old /content/instance_coordinates.txt\n",
            "Removed old /content/instance_demand.txt\n",
            "Removed old /content/instance_service_time.txt\n",
            "Removed old /content/instance_time_windows.txt\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b22883e2-36b9-4ab3-afcb-27b227236913\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b22883e2-36b9-4ab3-afcb-27b227236913\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving instance_coordinates.txt to instance_coordinates.txt\n",
            "Saving instance_demand.txt to instance_demand.txt\n",
            "Saving instance_service_time.txt to instance_service_time.txt\n",
            "Saving instance_time_windows.txt to instance_time_windows.txt\n",
            "User uploaded file \"instance_coordinates.txt\" with length 3107 bytes\n",
            "User uploaded file \"instance_demand.txt\" with length 1697 bytes\n",
            "User uploaded file \"instance_service_time.txt\" with length 1293 bytes\n",
            "User uploaded file \"instance_time_windows.txt\" with length 3833 bytes\n",
            "\n",
            "All required files seem to be uploaded. Proceeding with the test run...\n",
            "\n",
            "Starting Column Generation Test (4 Customers with Uploaded Files)...\n",
            "Loading instance data...\n",
            "Calculating distance and travel time matrices...\n",
            "Loaded 201 customers\n",
            "Depot: 0\n",
            "Vehicle capacity: 250.0 kg\n",
            "Speed: 60.0 distance units/hour\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "(0, 1)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-d2d643f8e6a7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0mfeasible_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0;31m# Try departing early\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                 \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_route\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroute_sequence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcust_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeparture_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_4_cust\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcost\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                     \u001b[0mfeasible_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-a8d176869221>\u001b[0m in \u001b[0;36mevaluate_route\u001b[0;34m(route_sequence, departure_time, data)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# Calculate travel to customer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mtravel_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtravel_times\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustomer_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mtravel_distance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtravel_distances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustomer_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0marrival_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_time\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtravel_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: (0, 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure Gurobi is installed in your Colab environment:\n",
        "# !pip install gurobipy\n",
        "\n",
        "# Import necessary libraries (ensure these are also imported where your functions are defined)\n",
        "import math\n",
        "from typing import List, Dict, Tuple, Optional, Set, Any # Make sure these are imported in your function definitions too\n",
        "import gurobipy as gp\n",
        "from gurobipy import GRB\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# --- SAFETY CHECK: Ensure all required functions are defined ---\n",
        "# It's good practice to quickly check if the main functions exist before proceeding.\n",
        "# This won't check their internal correctness, just their presence.\n",
        "required_functions = [\n",
        "    \"load_and_preprocess_data\", \"evaluate_route\", \"column_generation_solver\",\n",
        "    \"Label\", \"initialize_depot_label\", \"_calculate_tau_ij_from_paper\",\n",
        "    \"_generate_candidate_intervals_at_j\", \"_filter_dominant_intervals_pareto\",\n",
        "    \"extend_label_to_new_node\", \"_calculate_phi_L1_L2\", \"check_dominance\",\n",
        "    \"solve_espprc_forward_labeling\", \"solve_restricted_master_problem\",\n",
        "    \"solve_master_ip\", \"solve_espprc_with_multiple_time_windows\"\n",
        "    # Add any other crucial functions your solver depends on\n",
        "]\n",
        "for func_name in required_functions:\n",
        "    if func_name not in globals():\n",
        "        print(f\"ERROR: Required function '{func_name}' is not defined in the notebook.\")\n",
        "        print(\"Please make sure all your function definition cells have been executed.\")\n",
        "        # You might want to raise an error here to stop execution\n",
        "        raise NameError(f\"Function {func_name} not defined.\")\n",
        "print(\"All prerequisite functions seem to be defined.\")\n",
        "\n",
        "# Step 1: File Upload Section\n",
        "print(\"\\n--- File Upload for Actual Run ---\")\n",
        "print(\"Please upload your actual instance files.\")\n",
        "print(\"Ensure they are named according to the prefix expected by 'load_and_preprocess_data'.\")\n",
        "\n",
        "# Define the expected file name prefix for your actual instance\n",
        "# For example, if your files are \"actual_coords.txt\", \"actual_demand.txt\",\n",
        "# then INSTANCE_NAME_PREFIX_ACTUAL_RUN would be \"actual\"\n",
        "INSTANCE_NAME_PREFIX_ACTUAL_RUN = input(\"Enter the common prefix for your instance files (e.g., 'instance', 'problemX'): \")\n",
        "\n",
        "# Clean up any old files if they exist in /content/ that match the new prefix\n",
        "# (Be careful with this if you have other important files with the same prefix)\n",
        "files_to_potentially_remove = [\n",
        "    f\"{INSTANCE_NAME_PREFIX_ACTUAL_RUN}_coordinates.txt\",\n",
        "    f\"{INSTANCE_NAME_PREFIX_ACTUAL_RUN}_demand.txt\",\n",
        "    f\"{INSTANCE_NAME_PREFIX_ACTUAL_RUN}_service_time.txt\",\n",
        "    f\"{INSTANCE_NAME_PREFIX_ACTUAL_RUN}_time_windows.txt\"\n",
        "]\n",
        "for f_name in files_to_potentially_remove:\n",
        "    if os.path.exists(f\"/content/{f_name}\"):\n",
        "        print(f\"Found existing file: /content/{f_name}. Removing it before upload...\")\n",
        "        os.remove(f\"/content/{f_name}\")\n",
        "\n",
        "uploaded_actual = files.upload()\n",
        "\n",
        "for fn_actual in uploaded_actual.keys():\n",
        "  print(f'User uploaded file \"{fn_actual}\" with length {len(uploaded_actual[fn_actual])} bytes')\n",
        "\n",
        "# Verify uploaded files\n",
        "all_files_present_actual = True\n",
        "required_files_actual = [\n",
        "    f\"{INSTANCE_NAME_PREFIX_ACTUAL_RUN}_coordinates.txt\",\n",
        "    f\"{INSTANCE_NAME_PREFIX_ACTUAL_RUN}_demand.txt\",\n",
        "    f\"{INSTANCE_NAME_PREFIX_ACTUAL_RUN}_service_time.txt\",\n",
        "    f\"{INSTANCE_NAME_PREFIX_ACTUAL_RUN}_time_windows.txt\"\n",
        "]\n",
        "for req_file_actual in required_files_actual:\n",
        "    if not os.path.exists(f\"/content/{req_file_actual}\"):\n",
        "        print(f\"ERROR: Required file /content/{req_file_actual} was not uploaded or has a different name.\")\n",
        "        all_files_present_actual = False\n",
        "\n",
        "if not all_files_present_actual:\n",
        "    print(\"Please re-run the cell and ensure all required files for the actual instance are uploaded with correct names.\")\n",
        "else:\n",
        "    print(\"\\nAll required files for the actual run seem to be uploaded.\")\n",
        "    print(\"Proceeding with the solver execution...\")\n",
        "\n",
        "    # Step 2: Define Parameters for the Actual Run\n",
        "    # These might be different from your test runs.\n",
        "    # Your problem description mentions:\n",
        "    # - 200 retailers (customers) - your instance files will reflect this\n",
        "    # - Vehicle capacity Q = 12600 kg\n",
        "    # - Speed of trucks = 1000 distance units / h\n",
        "\n",
        "    actual_depot_id = int(input(f\"Enter the DEPOT ID for instance '{INSTANCE_NAME_PREFIX_ACTUAL_RUN}' (e.g., 0 or 1): \"))\n",
        "    actual_vehicle_capacity = float(input(f\"Enter the VEHICLE CAPACITY for instance '{INSTANCE_NAME_PREFIX_ACTUAL_RUN}' (e.g., 12600): \") or 12600.0)\n",
        "    actual_speed_dist_per_hr = float(input(f\"Enter the vehicle SPEED (distance units per HOUR) for instance '{INSTANCE_NAME_PREFIX_ACTUAL_RUN}' (e.g., 1000): \") or 1000.0)\n",
        "    actual_alpha_penalty = float(input(f\"Enter the ALPHA PENALTY (cost per vehicle) for instance '{INSTANCE_NAME_PREFIX_ACTUAL_RUN}' (e.g., 1000, 5000): \") or 1000.0) # Adjust as needed\n",
        "    actual_max_cg_iter = int(input(f\"Enter the MAXIMUM Column Generation ITERATIONS for instance '{INSTANCE_NAME_PREFIX_ACTUAL_RUN}' (e.g., 50, 100): \") or 50)\n",
        "    actual_depot_planning_end_time_min = float(input(f\"Enter the DEPOT latest return time (minutes from midnight, e.g., 1440 for 24h): \") or 24*60)\n",
        "\n",
        "    # Step 3: Load and Preprocess Data for the Actual Run\n",
        "    print(f\"\\nLoading and preprocessing data for instance: {INSTANCE_NAME_PREFIX_ACTUAL_RUN}...\")\n",
        "    actual_instance_data = None\n",
        "    try:\n",
        "        actual_instance_data = load_and_preprocess_data(\n",
        "            base_path=\"/content\",\n",
        "            instance_name=INSTANCE_NAME_PREFIX_ACTUAL_RUN,\n",
        "            speed=actual_speed_dist_per_hr,\n",
        "            vehicle_capacity=actual_vehicle_capacity,\n",
        "            depot_id=actual_depot_id\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error during data loading and preprocessing: {e}\")\n",
        "        print(\"Please check your uploaded files and their format.\")\n",
        "\n",
        "    if actual_instance_data and actual_instance_data.get('customers_list'):\n",
        "        print(f\"\\nData loaded successfully. Number of customers: {len(actual_instance_data['customers_list'])}\")\n",
        "\n",
        "        # Step 4: Prepare Initial Routes for the Actual Run\n",
        "        # For a large instance (200 customers), creating good initial routes can be complex.\n",
        "        # Starting with single-customer routes is a safe but potentially slow start for CG.\n",
        "        # You might have a separate heuristic to generate better starting routes.\n",
        "        # For now, let's stick to single-customer routes, but be aware this might be insufficient for very large problems.\n",
        "        print(\"\\nPreparing initial routes (single customer per route)...\")\n",
        "        initial_routes_actual_run = []\n",
        "        for cust_id_actual in actual_instance_data['customers_list']:\n",
        "            feasible_cost_actual = None\n",
        "            # Try a few departure times\n",
        "            departure_attempts = [0] # Start with departing at time 0\n",
        "            cust_tws_actual = actual_instance_data['time_windows_data'].get(cust_id_actual, [])\n",
        "            if cust_tws_actual: # Try to depart just in time for each window\n",
        "                travel_to_cust_actual = actual_instance_data['travel_times'].get((actual_depot_id, cust_id_actual), 0)\n",
        "                for tw_start, _ in cust_tws_actual:\n",
        "                    departure_attempts.append(max(0, tw_start - travel_to_cust_actual))\n",
        "\n",
        "            for dep_time in sorted(list(set(departure_attempts))): # Unique sorted departure attempts\n",
        "                cost_actual = evaluate_route(route_sequence=[cust_id_actual], departure_time=dep_time, data=actual_instance_data)\n",
        "                if cost_actual is not None:\n",
        "                    if feasible_cost_actual is None or cost_actual < feasible_cost_actual:\n",
        "                        feasible_cost_actual = cost_actual\n",
        "\n",
        "            if feasible_cost_actual is not None:\n",
        "                initial_routes_actual_run.append({\n",
        "                    'id': f'initial_actual_cust_{cust_id_actual}',\n",
        "                    'cost': feasible_cost_actual,\n",
        "                    'customers_served': [cust_id_actual]\n",
        "                })\n",
        "                # print(f\"Initial route for actual customer {cust_id_actual} cost: {feasible_cost_actual:.2f}\")\n",
        "            else:\n",
        "                # Fallback if evaluate_route fails for all attempts\n",
        "                dist_to_actual = actual_instance_data['travel_distances'].get((actual_depot_id, cust_id_actual), float('inf'))\n",
        "                dist_from_actual = actual_instance_data['travel_distances'].get((cust_id_actual, actual_depot_id), float('inf'))\n",
        "                if dist_to_actual != float('inf') and dist_from_actual != float('inf'):\n",
        "                    raw_cost_actual = dist_to_actual + dist_from_actual\n",
        "                    initial_routes_actual_run.append({\n",
        "                        'id': f'initial_actual_cust_{cust_id_actual}_raw',\n",
        "                        'cost': raw_cost_actual,\n",
        "                        'customers_served': [cust_id_actual]\n",
        "                    })\n",
        "                    print(f\"Warning: Could not find time-feasible initial route for Cust {cust_id_actual}. Using raw distance: {raw_cost_actual:.2f}\")\n",
        "                else:\n",
        "                     print(f\"ERROR: Cannot even calculate raw distance for initial route for Cust {cust_id_actual}.\")\n",
        "\n",
        "\n",
        "        if not initial_routes_actual_run and actual_instance_data['customers_list']:\n",
        "            print(\"Error: No initial routes could be created for the actual run, but customers exist. Aborting solver.\")\n",
        "        elif not actual_instance_data['customers_list']:\n",
        "             print(\"No customers loaded. Nothing to solve.\")\n",
        "        else:\n",
        "            print(f\"\\nSuccessfully created {len(initial_routes_actual_run)} initial routes.\")\n",
        "            print(\"Starting the Column Generation Solver for the actual run...\")\n",
        "\n",
        "            # Step 5: Run the Column Generation Solver\n",
        "            # ENSURE column_generation_solver DEFINITION ACCEPTS depot_id and depot_max_time\n",
        "            final_model_actual, selected_routes_actual, all_generated_routes_actual = column_generation_solver(\n",
        "                initial_routes=initial_routes_actual_run,\n",
        "                customers_list=actual_instance_data['customers_list'], # Corrected: no _cg\n",
        "                demands=actual_instance_data['demands'],                 # Corrected\n",
        "                service_times=actual_instance_data['service_times'],     # Corrected\n",
        "                time_windows_data=actual_instance_data['time_windows_data'], # Corrected\n",
        "                coordinates_data=actual_instance_data['coordinates_data'], # Corrected\n",
        "                vehicle_capacity=actual_vehicle_capacity,               # Corrected\n",
        "                speed=actual_speed_dist_per_hr,                         # Corrected\n",
        "                alpha_penalty=actual_alpha_penalty,                     # Corrected\n",
        "                max_cg_iterations=actual_max_cg_iter,\n",
        "                depot_id_cg=actual_depot_id,                               # Pass explicitly\n",
        "                depot_max_time_cg=actual_depot_planning_end_time_min       # Pass explicitly\n",
        "            )\n",
        "\n",
        "            # Step 6: Report Results\n",
        "            if selected_routes_actual:\n",
        "                print(\"\\n--- Actual Run Completed Successfully ---\")\n",
        "                num_vehicles_actual = len(selected_routes_actual)\n",
        "                total_dist_actual = sum(r['cost'] for r in selected_routes_actual)\n",
        "                print(f\"Number of vehicles used: {num_vehicles_actual}\")\n",
        "                print(f\"Total travel distance: {total_dist_actual:.2f}\")\n",
        "\n",
        "                if final_model_actual and hasattr(final_model_actual, 'ObjVal') and final_model_actual.SolCount > 0:\n",
        "                     calculated_obj_actual = total_dist_actual + actual_alpha_penalty * num_vehicles_actual\n",
        "                     print(f\"Calculated Objective (dist + alpha*num_veh): {calculated_obj_actual:.2f}\")\n",
        "                     print(f\"Gurobi Master IP Objective: {final_model_actual.ObjVal:.2f}\")\n",
        "                     # Optimality Gap (if RMP provides a valid lower bound from the last iteration)\n",
        "                     # This requires capturing the final RMP objective before the IP solve.\n",
        "                     # The current CG function doesn't return it, but it's printed.\n",
        "                     # For a true gap, you'd need the final LP relaxation value of the full master problem.\n",
        "                     # If final_ip_model.ObjBound is available and meaningful:\n",
        "                     if hasattr(final_model_actual, 'ObjBound') and final_model_actual.ObjVal > 0: # Ensure ObjVal is positive for meaningful gap\n",
        "                         gap = ( (final_model_actual.ObjVal - final_model_actual.ObjBound) / final_model_actual.ObjVal ) * 100\n",
        "                         print(f\"Optimality Gap (from Gurobi IP solve): {gap:.2f}% (ObjBound: {final_model_actual.ObjBound:.2f})\")\n",
        "                     else:\n",
        "                         print(\"Optimality Gap information not readily available from Gurobi IP solve for this run.\")\n",
        "\n",
        "\n",
        "                print(\"\\nSelected Routes Details:\")\n",
        "                for i, r_detail_actual in enumerate(selected_routes_actual):\n",
        "                     print(f\"  Route {i+1} (ID {r_detail_actual.get('id', 'N/A')}):\")\n",
        "                     print(f\"    Customers: {r_detail_actual['customers_served']}\")\n",
        "                     print(f\"    Travel Distance (cost): {r_detail_actual['cost']:.2f}\")\n",
        "                     # You might want to re-evaluate each selected route here with evaluate_route\n",
        "                     # to confirm its full schedule and feasibility.\n",
        "            elif final_model_actual:\n",
        "                 print(\"\\n--- Actual Run Did Not Produce a Feasible Solution via Master IP ---\")\n",
        "                 print(f\"Final IP Model status: {final_model_actual.status}\")\n",
        "            else:\n",
        "                print(\"\\n--- Actual Run Did Not Produce a Final Solution (CG or IP failed more fundamentally) ---\")\n",
        "\n",
        "            if all_generated_routes_actual:\n",
        "                print(f\"\\nTotal routes in pool at end of CG: {len(all_generated_routes_actual)}\")\n",
        "            else:\n",
        "                print(\"\\nNo routes in pool at end (or CG did not complete to that stage).\")\n",
        "    else:\n",
        "        print(\"\\n--- Actual Run Aborted: Data loading failed or no customers found. ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dSnfcFlyV_K3",
        "outputId": "8ec72d69-3ff2-4ffb-e340-92cb38c860b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All prerequisite functions seem to be defined.\n",
            "\n",
            "--- File Upload for Actual Run ---\n",
            "Please upload your actual instance files.\n",
            "Ensure they are named according to the prefix expected by 'load_and_preprocess_data'.\n",
            "Enter the common prefix for your instance files (e.g., 'instance', 'problemX'): instance\n",
            "Found existing file: /content/instance_coordinates.txt. Removing it before upload...\n",
            "Found existing file: /content/instance_demand.txt. Removing it before upload...\n",
            "Found existing file: /content/instance_service_time.txt. Removing it before upload...\n",
            "Found existing file: /content/instance_time_windows.txt. Removing it before upload...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-deb71393-8876-4a94-ae3c-0fffc0f0f80b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-deb71393-8876-4a94-ae3c-0fffc0f0f80b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving instance_coordinates.txt to instance_coordinates.txt\n",
            "Saving instance_demand.txt to instance_demand.txt\n",
            "Saving instance_service_time.txt to instance_service_time.txt\n",
            "Saving instance_time_windows.txt to instance_time_windows.txt\n",
            "User uploaded file \"instance_coordinates.txt\" with length 3107 bytes\n",
            "User uploaded file \"instance_demand.txt\" with length 1697 bytes\n",
            "User uploaded file \"instance_service_time.txt\" with length 1293 bytes\n",
            "User uploaded file \"instance_time_windows.txt\" with length 3833 bytes\n",
            "\n",
            "All required files for the actual run seem to be uploaded.\n",
            "Proceeding with the solver execution...\n",
            "Enter the DEPOT ID for instance 'instance' (e.g., 0 or 1): 1\n",
            "Enter the VEHICLE CAPACITY for instance 'instance' (e.g., 12600): 12600\n",
            "Enter the vehicle SPEED (distance units per HOUR) for instance 'instance' (e.g., 1000): 1000\n",
            "Enter the ALPHA PENALTY (cost per vehicle) for instance 'instance' (e.g., 1000, 5000): 1000\n",
            "Enter the MAXIMUM Column Generation ITERATIONS for instance 'instance' (e.g., 50, 100): 100\n",
            "Enter the DEPOT latest return time (minutes from midnight, e.g., 1440 for 24h): 1440\n",
            "\n",
            "Loading and preprocessing data for instance: instance...\n",
            "Loading instance data...\n",
            "Calculating distance and travel time matrices...\n",
            "Loaded 200 customers\n",
            "Depot: 1\n",
            "Vehicle capacity: 12600.0 kg\n",
            "Speed: 1000.0 distance units/hour\n",
            "\n",
            "Data loaded successfully. Number of customers: 200\n",
            "\n",
            "Preparing initial routes (single customer per route)...\n",
            "Warning: Could not find time-feasible initial route for Cust 19. Using raw distance: 2899.21\n",
            "Warning: Could not find time-feasible initial route for Cust 35. Using raw distance: 2771.78\n",
            "Warning: Could not find time-feasible initial route for Cust 51. Using raw distance: 2721.21\n",
            "Warning: Could not find time-feasible initial route for Cust 54. Using raw distance: 2705.21\n",
            "Warning: Could not find time-feasible initial route for Cust 58. Using raw distance: 2691.38\n",
            "Warning: Could not find time-feasible initial route for Cust 65. Using raw distance: 2599.67\n",
            "Warning: Could not find time-feasible initial route for Cust 67. Using raw distance: 2586.48\n",
            "Warning: Could not find time-feasible initial route for Cust 68. Using raw distance: 2577.04\n",
            "Warning: Could not find time-feasible initial route for Cust 74. Using raw distance: 2520.18\n",
            "Warning: Could not find time-feasible initial route for Cust 79. Using raw distance: 2482.40\n",
            "Warning: Could not find time-feasible initial route for Cust 87. Using raw distance: 2444.57\n",
            "Warning: Could not find time-feasible initial route for Cust 109. Using raw distance: 2152.14\n",
            "Warning: Could not find time-feasible initial route for Cust 111. Using raw distance: 2105.71\n",
            "Warning: Could not find time-feasible initial route for Cust 128. Using raw distance: 1863.25\n",
            "Warning: Could not find time-feasible initial route for Cust 145. Using raw distance: 1759.18\n",
            "Warning: Could not find time-feasible initial route for Cust 172. Using raw distance: 1331.89\n",
            "Warning: Could not find time-feasible initial route for Cust 178. Using raw distance: 1159.23\n",
            "Warning: Could not find time-feasible initial route for Cust 179. Using raw distance: 1135.96\n",
            "Warning: Could not find time-feasible initial route for Cust 201. Using raw distance: 341.58\n",
            "\n",
            "Successfully created 200 initial routes.\n",
            "Starting the Column Generation Solver for the actual run...\n",
            "CG: Pre-calculating distance and travel time matrices...\n",
            "CG: Matrices calculated.\n",
            "\n",
            "--- Column Generation Iteration: 1 ---\n",
            "Current number of routes in RMP: 200\n",
            "Set parameter WLSAccessID\n",
            "Set parameter WLSSecret\n",
            "Set parameter LicenseID to value 2652745\n",
            "Set parameter Threads to value 95\n",
            "Academic license 2652745 - for non-commercial use only - registered to di___@student.sdu.dk\n",
            "Gurobi Optimizer version 12.0.2 build v12.0.2rc0 (linux64 - \"Ubuntu 22.04.5 LTS\")\n",
            "\n",
            "CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz, instruction set [SSE2|AVX|AVX2|AVX512]\n",
            "Thread count: 48 physical cores, 96 logical processors, using up to 95 threads\n",
            "\n",
            "Non-default parameters:\n",
            "Threads  95\n",
            "\n",
            "Academic license 2652745 - for non-commercial use only - registered to di___@student.sdu.dk\n",
            "Optimize a model with 200 rows, 200 columns and 200 nonzeros\n",
            "Model fingerprint: 0xf330faa1\n",
            "Coefficient statistics:\n",
            "  Matrix range     [1e+00, 1e+00]\n",
            "  Objective range  [1e+03, 4e+03]\n",
            "  Bounds range     [0e+00, 0e+00]\n",
            "  RHS range        [1e+00, 1e+00]\n",
            "Presolve removed 200 rows and 200 columns\n",
            "Presolve time: 0.01s\n",
            "Presolve: All rows and columns removed\n",
            "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
            "       0    6.1881592e+05   0.000000e+00   0.000000e+00      0s\n",
            "\n",
            "Solved in 0 iterations and 0.01 seconds (0.00 work units)\n",
            "Optimal objective  6.188159200e+05\n",
            "RMP Objective Value: 618815.92\n"
          ]
        }
      ]
    }
  ]
}